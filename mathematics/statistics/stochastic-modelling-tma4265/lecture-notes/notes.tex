\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Stochastic Modelling}
\author{isakhammer }
\date{2020}

%%%% DEPENDENCIES v1.1 %%%%%%

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{enumerate}
\usepackage{todonotes}
\usepackage{float}


\usepackage{hyperref} 
\hypersetup{
  colorlinks=true, %set true if you want colored links
  linktoc=all,     %set to all if you want both sections and subsections linked
  linkcolor=blue,  %choose some color if you want links to stand out
} 
\hypersetup{linktocpage}


% inscape-figures
\usepackage{import}
\usepackage{pdfpages}
\usepackage{transparent}
\usepackage{xcolor}
\newcommand{\incfig}[2][1]{%
\def\svgwidth{#1\columnwidth}
\import{./figures/}{#2.pdf_tex} } \pdfsuppresswarningpagegroup=1

% Box environment
\usepackage{tcolorbox}
\usepackage{mdframed}
\newmdtheoremenv{definition}{Definition}[section]
\newmdtheoremenv{theorem}{Theorem}[section]
\newmdtheoremenv{lemma}{Lemma}[section]

% \DeclareMathOperator{\span}{span}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
%\newtheorem{example}{Example}

\newcommand{\newpara}
  {
  \vskip 0.4cm
  }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{document}
\maketitle
\tableofcontents
\newpage

\newpage
\section{Lecture 1}%
\label{sec:lecture_1}

\subsection{Practical Information}%
\label{sub:practical_information}

Two projects 
\begin{itemize}
  \item The projects count $20 \%$ and exam $80 \%$.
  \item Must be done with two people.
  \item If you want to do statistics is it worth learning $R$.
\end{itemize}

\textbf{Course Overview} 
\begin{itemize}
  \item Markov chains for discret time and discrete outcome.
    \begin{itemize}
      \item Set of states and discrete time points.
      \item Transition between states
      \item Future depends on the present, but not the past.
    \end{itemize}
  \item Continious time Markoc chains. (continious time and discrete toutcome.
  \item Brownian motion and Gaussian processes (continionus time and continious outcome.)
\end{itemize}


\subsection{Mathematical description}%
\label{sub:mathematical_description}
 \begin{definition}
   A \textbf{stochastic process} $\{ x\left( t \right), t \in T\} $ is a family of random variables, where $T$ is a set of indicies, and $X\left( t \right)$ is a random variable for each value of $t$.
 \end{definition}

\subsection{Recall from Statistics Course}%
\label{sub:recall_from_statistics_course}

A random experiment is perfomed the outcome of the experiment is random.
\begin{itemize}
  \item THe set of possible outcomes is the \textbf{sample space}  $\omega $ 
    \begin{itemize}
      \item An \textbf{event}  $A \subset \omega $  if the outcome is contained in $A$
      \item The \textbf{complement}  of an event $A$ is  $A^{c} = \omega  \setminus A$ 
      \item The \textbf{null event} $\emptyset$ is the empty set $\emptyset = \omega \setminus \omega $ 
    \end{itemize}
\end{itemize}

\subsubsection{Combining Event}%
\label{ssub:combining_event}

Let $A$ and B be events 
\begin{itemize}
  \item The \textbf{union} $A \cup  B$ is the event that at least one of $A$ and $B$ occur.
  \item the \textbf{intersection}  $A \cap B$ is the event that both $A$ and $B$ occur.
\end{itemize}

The events $A_{1}, A_{2}, \ldots$ are called disjoint (or \textbf{mutually exclusive} ) if $A_{i} \cap A_{j} = \emptyset$ for $i \neq j$

\subsubsection{Probability}%
\label{ssub:probability}

$Pr$ is called a probability on $\omega $ if 

\begin{itemize}
  \item Pr $\{ \omega \} = 1  $ 
  \item $0 \le P\left\{ A \right\} \le 1$ for all events $A$ 
  \item For $A_{1}, A_{2} , \ldots$ that are mutually exclusive \[
  P \left\{ \bigcup_{i = 1}^{\infty}A_{i}  \right\} = \sum_{i=1}^{\infty} P \left\{ A_{i} \right\}
  \] 
\end{itemize}
We call $P\left\{ A \right\}$ the probability of $A$.


\subsubsection{Law of total probability}%
\label{ssub:law_of_total_probability}

Let $A_{1}, A_{2}, \ldots$ be a partition of $\omega $ ie 
\begin{itemize}
  \item $\omega  = \bigcup_{i=1}^{\infty} A_{i}$
  \item $A_{1}, A_{2}, A_{3}, \ldots$ are mutually exclusive.
\end{itemize}

Then for any event $B$ \[
  P\left\{ B \right\} = \sum_{i=1}^{\infty} P\left\{ B \cap A_{i} \right\}
\] 

\textbf{This concept is very important.} 

\subsubsection{Independence}%
\label{ssub:independence_2s}
Event $A$ and $B$ are independent of \[
P\left\{ A\cap B \right\} = P\left\{ A \right\}P\left\{ B \right\}
\] 
Events $A_{1}, \ldots, A_{n}$ are independent if for any subset \[
P\left\{ \bigcap_{j=1}^{k} A_{i_j} \right\} = \prod_{j=1}^{k} P \left\{ A_{i_j} \right\} 
\] 

In this case $P\left\{ \bigcap_{i = 1}^{n} A_{1} \right\} =  \prod_{i = 1}^{n} P\left\{ A_{i} \right\} $


\newpage
\subsubsection{Random Variables}%
\label{ssub:random_variables}

\begin{definition}
  A \textbf{random variable}  is a real-vaued function on the sample space. Informally:  A random variable is a real valued variable that takes on its value by chance.
\end{definition}


\begin{tcolorbox}
  \textbf{Example.} 
  \begin{itemize}
    \item Throw two dice. $X = \text{sum of the two dice}$
    \item Throw a coin.  $X$ is $1$ for heads and $X$ is $0$ for tails.
  \end{itemize}
\end{tcolorbox}


\subsubsection{Notation for random variables}%
\label{ssub:notation_for_random_variables}

We use 
\begin{itemize}
  \item upper case letters such at $X$, $Y$ and $Z$  to represent random variables.
  \item lower case letters as $x$, $y$, $z$ to denote the real-valued realized value of a the random variable.
\end{itemize}

Expression such as $\left\{ X \le x \right\}$ denators the event that $X$ assumes a valye less than or earl to the real number x.

\subsubsection{Discrete random variables}%
\label{ssub:discrete_random_variables}

The random variable $X$ is \textbf{discrete}  if it has a finite or countablle number of possible outcomes $x_{1}, x_{2}, \ldots$ \par
\begin{itemize}
  \item The \textbf{probability mass function } $p_{x} \left( x \right) $ is given by \[
  p_{x}\left( x \right) = P \left\{ X = x \right\}
  \] and satisfies \[
  \sum_{i=1}^{\infty} p_{x}\left( x_{i} \right) = 1 \quad  \text{and} \quad  0\le p_{x} \left( x_{i} \right) \le  1 
  \] 
\item The \textbf{cumulative distribution function} (CDF) a of $X$ can be written \[
F_{x}\left( x \right) = P\left\{ X \le x \right\} = \sum_{i: x_{i} \le x}^{} p_{x}\left( x_{i} \right) 
\]  
\end{itemize}

\subsubsection{CFD}%
\label{ssub:cfd} 

The CDF of $X$ may also be called the \textbf{distrobution function}  of $X$ \par 
Let $F_{x}\left( x \right)$ be the CDF of $X$, then 
\begin{itemize}
  \item $F_{x}\left( x \right)$ is monetonaly increasing.
  \item $F_{x}$ is a stepfunction, which is a pieace-wise constant with jumps at $x_{i}.$
  \item $\lim_{x \to \infty} F_{x}\left( x \right) = 1$
  \item $\lim_{x \to - \infty} F_{x}\left( x \right) = 0$
\end{itemize}


\subsubsection{Continious random vairbales}%
\label{ssub:continious_random_vairbales}
 A \textbf{continious} random variables takes value o a continious scale.
 \begin{itemize}
   \item The CDF, $F_{x}\left( x \right) = P \left( X \le x \right)$ is continious.
   \item The \textbf{probability density function} (PDF) $f_{x}\left( x \right) = F_{x}' \left( x \right)$ can be used to calculate probablities \[
   \begin{split}
     Pr \left\{ a < X < b \right\} &=  Pr \left\{ a \le X < b \right\} = Pr\left\{ a < X \le b \right\} \\
     &=  Pr\left\{ a \le X \le b \right\} = \int_{a}^{b}  f_{x}\left( x \right)dx   
   \end{split} 
   \] 
 \end{itemize}


 \subsubsection{Important properties}%
 \label{ssub:important_properties}

 \begin{itemize}
   \item CDF:
     \begin{itemize}
       \item Monotonely increaing
       \item continious
        \item $\lim_{x \to \infty} F_{x} = 1$ and $\lim_{x \to - \infty} F_{x}\left( x \right) = 0$
     \end{itemize}
   \item PDF
     \begin{itemize}
       \item $f_{x}\left( x \right) \ge 0$ for $x \in\mathbb{R} $
       \item $\int_{-\infty}^{\infty} f_{x}\left( x \right)dx = 1$
     \end{itemize}
 \end{itemize}


\subsubsection{Expectation}%
\label{ssub:expectation}

Let $g: \mathbb{R}  \to \mathbb{R} $ be a function and $X$ be a random variable.
\begin{itemize}
  \item If $X$ is discrete, the expected value of $g\left( X \right) $ is \[
  E\left[ g\left( X \right) \right] =  \sum_{x: p_{x}\left( x \right)> 0}^{} g\left( x \right) p_{x}\left( x \right)  
  \] 
\item If $X$ is continous, the expected value of $g\left( X \right) $ is  \[
E\left[ g\left( X \right) \right] = \int_{-\infty}^{\infty} g\left( x \right)f_{x}\left( x \right) dx 
\] 
\end{itemize}

\subsubsection{Variance}%
\label{ssub:variance}

The variance of the random variable $X$ is \[
  Var\left[ X \right] =  E \left[( X - E\left[ X \right])^{2} \right] =  E\left[ X^2 \right] - E\left[ X \right]^2 
\] 
Important properties of expectation and variance.
\begin{itemize}
  \item Expectations is linear \[
  E\left[ aX + bY +c \right] = aE\left[ X \right] + bE\left[ Y \right] + c.
  \] 
\item Variance scales quadratically and is invaraient to the addition of constants \[
Var\left[ aX + b \right] = a^2 Var \left[ X \right] 
\] 
\item fir independent stochastic variables.\[
    Var \left[ X + Y \right] = Var \left[ X \right] + Var\left[ Y \right]
\] 
\end{itemize}

\subsubsection{Joint CDF}%
\label{ssub:joint_cdf}

If $\left( X,Y \right)$ is a pair for random variables, their \textbf{joint comulative distribution function } is given by \[
F_{X,Y} = F\left( x,y \right) =  Pr\left\{ X \le x \cap Y \le y \right\}
\]. 
\subsubsection{Joint distrubution for discrete random variables}%
\label{ssub:joint_distrobution_for_discrete_random_variables}
If $X$ and $Y$  are discrete, the \textbf{joint probability mass function } $ p_{x,y} = Pr\left\{ X = x, Y =y \right\} $. can be used to compute probabilities \[
Pr\left\{ a < X < b, c < Y \le d \right\} =  \sum_{a < x \le b}^{}  \sum_{c < y \le d}^{} p_{X,Y}   \left( x,y \right)
\] 

\subsubsection{Joint distrubution for continous random variables}%
\label{ssub:joint_distrobution_for_continous_random_variables}

If $X$ and $Y$ are continious the \textbf{joint probability density function}  \[
.f_{X,Y} \left( x,y \right) = f\left( x,y \right) = \frac{\partial ^2}{\partial x \partial y } F\left( x,y \right)   
\]  can be used to compute probabilities \[
Pr\left\{ a < X \le b,  \quad  c < Y \le d  \right\} = \int_{a}^{b} \int_{c}^{d} f\left( x,y \right)dxdy    
\] 

\subsubsection{Independence}%
\label{ssub:independence_3}

The random variables $X$ and Y are independent if \[
Pr\left\{ X \le a , Y \le b \right\} =  Pr\left\{ X \le a \right\} \cdot  Pr\left\{ Y \le b \right\}, \quad  \forall a,b \in  \mathbb{R}  
\] 
In terms of CDFs:  $F_{X,Y}(a,b ) =  F_{X}\left( a \right)\cdot F_{Y}\left( b \right) \quad  \forall a,b \in \mathbb{R}  $
\par
Thus we have 
\begin{itemize}
  \item $p_{X,Y} \left( x,y \right) = p_{X}\left( x \right) \cdot  p_{Y}\left( Y \right)$ for discrete random variables
  \item $f_{X,Y}\left( x,y \right) = f_{X}\left( x \right) \cdot  f_{Y}\left( Y \right)$ for continuous random variables.
\end{itemize}






 
 




\newpage
\section{Lecture 3}%
\label{sec:lecture_3}

\subsection{Randoms sum}%
\label{sub:randoms_sum}

Building on the hunter example from last week. we can more generally consider random sums \[
  X = \begin{cases}
    0,  &  \quad  N = 0 \\
    \zeta_{1} + \zeta _{2} + \ldots + \zeta_N , \quad  N >0  
  \end{cases}
\] 
where 
\begin{itemize}
  \item $N$ is a discrete random variable with values $0,1, \ldots$ 
  \item $\zeta _{1}, \zeta _{2}, \ldots $ are independent random variables
  \item $N$ is independent of $\zeta _{1}, \zeta _{2} + \ldots + \zeta _{N}$ 
  \item \textbf{Notation}  $X = \sum_{i=1}^{N} \zeta _{i} = \zeta _{1} + \zeta _{2} + \ldots + \zeta _{N}$ 
\end{itemize}

\begin{tcolorbox}
  \textbf{Example.} 
  \begin{enumerate}
    \item Insurance company \[
    N: \text{ Number of claims.} 
    \] 
  \[
    \zeta _{1} , \zeta _{2} , \ldots \quad  : \quad \text{Sizes of the claims} 
  \] 

  Total liabilility: \[
  X = \zeta _{1}+ \zeta _{2} + \ldots + \zeta _{N}
  \] 
\item  Be careful! \[
    \begin{split}
      \overbrace{E\left[ \sum_{i=1}^{N} \zeta _{i} \right]}^{\neq \sum_{i=1}^{N} E\left[ \zeta _{i} \right]}   & = E\left[ E\left[ \sum_{i=1}^{N} \zeta _{i}  \mid N \right] \right]\\
&= E\left[ \sum_{i=1}^{N} E\left[ \zeta _{i}  \mid  N \right] \right] 
    \end{split} 
\] 
  \end{enumerate}
\end{tcolorbox}

\subsection{Self Study}%
\label{sub:self_study}

Section 2.2, 2.3, 2.4

\subsection{Stochastic process in descrete time}%
\label{sub:stochastic_process_in_descrete_time}
\begin{definition}
  A \textbf{discrete-time stochastic process}  is a family of random variables $\left[ X_{t} : t \in  T \right]$ where $T$ is discrete.
  \begin{itemize}
    \item We use $T = \left\{ 0,1,2,.. \right\}$ and write $X_{n}$ instead of $X_{t}$
    \item  we call $X_{n}$ the \textbf{state}  at time $n =  0,1,2,3, \ldots$
    \item We call the set of all possible states the \textbf{state space} 
  \end{itemize}
\end{definition}

\begin{table}[htpb]
  \centering
  \caption{Table for example}
  \label{tab:label}
  \begin{tabular}{l|cccc}
    Day & $n =0$ & $n=1$ & $n=2$ & \ldots \\ 
    Random Variable  & $X_{0} $ & $X_{1}$ & $X_{2}$ & \ldots \\
    Realization  1& $x_{0} = 0$ & $x_{1} =1$ &  $x_{2} = 1 $ & \ldots \\
    Realization 2 & $x_{0} = 1$ & $x_{1} =1$ &  $x_{2} = 1 $ & \ldots \\
  \end{tabular}
\end{table}
\begin{tcolorbox}
  \textbf{Example.}  \[
  X_{n} = \begin{cases}
    1 ,  &  \quad \text{if it rains on day } n \\
    0,   &  \quad     \text{no rain on day } n
  \end{cases}
  \] 
  State space $= \left\{ 0,1 \right\}$
  \par
  \textbf{We have a problem.} Need \[
  Pr \left \{ X_{n} = x_{n}  \mid  X_{n-1} = x_{n} , X_{n-2} = x_{n-2}, \ldots, X_{0} = x_{0} \right \}.
  \]    for all $n = 0,1,2,\ldots$

\end{tcolorbox}

\subsection{Markov chain}%
\label{sub:markov_chain}


\begin{definition}[Discrete time Markov Chain]
  A \textbf{ Discrete time markoc chain}  is a discrete time stochastic process $\left\{ X_{n} : n = 0,1,\ldots \right\}$ that statisfied the \textbf{markov property}  such that \[
  \begin{split}
       & Pr \left \{ X_{n-1} = j  \mid  X_{n} = i ,    X_{n-1} = i_{n-1} , \ldots, X_{0} = i_{0} \right \}  \\
    &=  Pr \left \{ X_{n+1} = j  \mid  X_{n} = i \right \}  
  \end{split} 
  \] 
  for $n = 0,1,2,3, \ldots$ and for all states $i$ and $j$
\end{definition}

\begin{definition}[One-step transition probabilities]
  We can define it  as 
  \begin{itemize}
    \item For a discrete Markov chain $\left\{ X_{n}: n= 0,1,2, \ldots \right\}$ we call $P_{ij}^{n, n+1} = Pr \left \{ X_{n+1} = j , X_{n} =i \right \} $ the \textbf{one step trainsition probabilities} . 
    \item We will assume \textbf{stationary transition probabilities} , i.e that \[
    P_{ij}^{n, n+1} = P_{ij}
    \]   for $n = 0,1,2, \ldots$ and all states $i $ and $j$ . 
  \end{itemize}
\end{definition}

Some of the properties 
\begin{enumerate}
  \item "You will always go somewhere" \[
  \sum_{j}^{}  P_{ij} = 1 \quad  \forall i 
  \] 
\item The markov chain can be described as follows. \[
    \begin{split}
  & Pr \left \{ X_{0} = i_{0} , X_{1} = i_{1}, \ldots, X_{n} = i_{n} \right \}   \\
 &=  Pr \left \{ X_{0} = i_{0}  \right \}   Pr \left \{ X_{1} = i_{1}  \mid  X_{0} = i_{0} \right \}   \ldots \\
     & \quad \quad    Pr \left \{ X_{n} = x_{n}  \mid  X_{n-1} = i_{n-1} \ldots X_{0} = i_{0} \right \}  \\
  &  \quad \vdots \quad     \text{Markov step} \\
 &=  Pr \left \{ X_{0} = i_{0}  \right \}  \cdot  Pr \left \{ X_{1} = i_{1}  \mid X_{0} = i_{0} \right \} \ldots \\
  & \quad \quad    Pr \left \{ X_{n} = x_{n}  \mid  X_{n-1} = i_{n-1} \right \}   \\
 &=  Pr \left \{ X_{0} = i_{0}  \right \} P_{i_{0}, i_{1}} \cdot  P_{i_{1}, i_{2}} \ldots P_{i_{n-1}, i_{n}}
    \end{split} 
\] 
Which is a major simplification.
\end{enumerate}

\begin{definition}[Transition Probability Matrix] \quad
  For a discrete time markov-chain with state space $\left \{ 0,1, \ldots, N \right \}$ we call
  \[
  \mathbf{P} = \begin{bmatrix} 
    P_{00} & \ldots & P_{0N} \\
    P_{10}  & \ldots \\
    \vdots  &   &  \ddots \\
    P_{N0} & \ldots & P_{NN} 
  \end{bmatrix} 
  \] 
  Is the transition matrix.
  For statespace $\left\{ 0,1,2, \ldots \right\}$ we envision an infinitely sized matrix.
\end{definition}

 \begin{tcolorbox}
   \textbf{Example.} 
   \begin{itemize}
     \item Markoc chain : $\left\{ X_{n} : n = 0,1,2,\ldots \right\}$
     \item State space  $= \left\{ 0,1 \right\}$
     \item Transition Matrix \[
     \mathbf{P} = \begin{bmatrix} 
     0.9  &  0.1 \\
     0.6  &  0.4
     \end{bmatrix} 
     \] 
   \end{itemize}
   We can compute \[
     \begin{split}
        Pr \left \{ X_{3} = 1  \mid  X_{2} = 0 \right \} &=  p_{01} \\
        &= 0.1   \\
        Pr \left \{ X_{10} = 0  \mid  X_{9} = 1 \right \} &=  P_{10} \\
        &= 0.6  \\
     \end{split} 
   \] 
 \end{tcolorbox}

\begin{definition}[Transition Diagram]
  Let $\left\{ X_{n}: n = 0,1, \ldots \right\}$ be a discrete time Markov chain.  A \textbf{state trasnistion diagram} visualizes the transition probabilities as a weighted directed graph where the nodes are the states and the edges are the possible transitions marked with the transistion probabilities.
\end{definition}

\begin{tcolorbox}
  \textbf{Example.} State space $= \left\{ 0,1,2 \right\}$ and \[
  P = \begin{bmatrix} 
  0.95  & 0.05 & 9 \\
  0  & 0.9  &  0.1 \\
  0.01  &  0  &  0.99
  \end{bmatrix} 
  \] 
  Transisition diagram 
  \begin{tcolorbox}
    Nice figure of the diagram
  \end{tcolorbox}
\end{tcolorbox}

 \subsection{Doing n transitions.}%
 \label{sub:doing_n_transitions_}

 \begin{theorem}
   For a Markoc chain $\left\{ X_{n}: n= 0,1, \ldots \right\}$ and any $m\ge 0$ we have \[
     Pr \left \{ X_{m-n} = j  \mid X_{m} = i  \right \}  = P _{ij}^{(n)} =  \sum_{k=0}^{\infty}  P _{ik} P_{kj}^{(n-1)} ,  \quad  n>0 
   \] 
   where we define \[
   P_{ij}^{(0)} = \begin{cases}
     1 , \quad  i= j \\
     0, i \neq j 
   \end{cases}
   \] 
 \end{theorem}

 \begin{proof}
   Set $m = 0$ then is \[
   \begin{split}
     P_{ij }^{(n+1)}   & = Pr \left \{ X_{n+1} = j  \mid  X_{0} = i \right \}   \\
     &= \sum_{k}^{}  Pr \left \{ X_{n+1} = j, X_{1} = k  \mid  X_{0} = i \right \}   \\
     &=  \sum_{k}^{} Pr \left \{ X_{n+1} = j  \mid  X_{1} = k, X_{0} = i \right \} \cdot Pr \left \{ X_{1} = k  \mid  X_{0} = i \right \}   \\
     &= \sum_{k}^{} P_{kj}^{(h)} \cdot P_{ik}  = \sum_{k}^{}  P_{ik} P_{kj}^{(h)}
   \end{split} 
   \] 
 \end{proof}
 
 \begin{tcolorbox}
   \textbf{Example.} $\left\{ X_{n} : n= 0,1,2, \ldots \right\}$ is a markoc chain and \[
   P = \begin{bmatrix} 
   0.1  &  0.9 \\
   0.6  &  0.4 
   \end{bmatrix} 
   \] 
   Find $P_{01}^{(4)}$ .
   \textbf{Solution}. \[
   P^2 = \begin{bmatrix} 
   0.55  &  0.45 \\
   0.30  &  0.70
   \end{bmatrix} 
   \] 
   So by doing matrix multiplication and we end up with \[
   P^{4} = P^{2} \cdot  P^{2} = \begin{bmatrix} 
   0.4375  &  0.5625 \\
   0.3750  &  0.6250
   \end{bmatrix} 
   \] 
   Which therefore ends up with the answer \[
   P_{01}^{(4)} = 0.5625
   \] 
 \end{tcolorbox}
 




 \newpage
 \section{Lecture 4}%
 \label{sec:lecture_4}

 \subsection{Introduction to first step analysis}%
 \label{sub:introduction_to_first_step_analysis}

 \textbf{Input} 
 \begin{itemize}
   \item $i_{0}$ : starting state
    \item $P$ : transition probability matrix
    \item T: number of time steps
 \end{itemize}
 \textbf{Algorithm} 
 \begin{enumerate}
   \item Set $x_{0} = i_{0}$
   \item for $n=1 \ldots T$
   \item $\quad   $ Simulate $x_{n}$ from $X_{n}  \mid  X_{n-1} = x_{n-1}$
   \item end
 \end{enumerate}
 
 \textbf{output} : One realization $x_{0}, x_{1} , \ldots, x_{T}$ 
 
 \begin{tcolorbox}
   \textbf{Example.} 
   \[
   P = \begin{pmatrix}
   0.95  &  0.05  &  0 \\
   0  &  0.90  &  0.10 \\
   0.01  &  0  &  0.99
   \end{pmatrix} 
   \] 
   Let $x_{0} = 0$
   \begin{enumerate}
     \item $x_{0} = 0$  
     \item 
       \begin{align*}
       Pr \left \{ X_{1} = 0 | X_{0} = 0 \right \} = &  P_{00} = 0.95  \\
       Pr \left \{ X_{1}  \mid  X_{0}  \right \}  &=  P_{01} = 0.05 \\
       Pr \left \{ X_{1}  \mid  X_{0} = 0 \right \}  &=  P_{02} = 0 \\
       .\end{align*}
       Assume we get $x_{1} = 1$
     \item States 
       \begin{itemize}
         \item \[
             \begin{split}
         0: P_{10}  &=  0 \\
         1: P_{11 } &=  0.90 \\
         2: P_{12} &=  0.10 \\
         \vdots  \\
             \end{split} 
         \] 
       \end{itemize}
   \end{enumerate}
 \end{tcolorbox}

 \begin{tcolorbox}
   General notes on simulation 
   \begin{itemize}
     \item

   $Pr \left \{ A  \right \} \approx \frac{\text{# times A occure}}{ \text{# Simulations}}  $
 \item $E\left[ X \right] \approx \frac{1}{N}  \sum_{i=1}^{B}  x_{i}$
   \end{itemize}
 \end{tcolorbox}

   \textbf{Example.} We have $N=100$ divided into two containers labelled $A$ and $ b$. At each time $n$, one ball is selected at random and moved to the container. Let $Y_{n}$ denote the number of balls in container $A$ at time $n$, and define $X_{n} = Y_{n} -50$. Find the transition probabilities and simulate and plot one realization of \[
   \left\{ X_{n}: n  = 0,1, \ldots, 500 \right\}
   \] 

   \textbf{Answer} 
 
\begin{figure}[ht]
    \centering
    \incfig{balls}
    \caption{balls}
    \label{fig:balls}
\end{figure}

\begin{itemize}
  \item Only move One ball
  \item Can move only from $i$ to $ j = i-1$ or  $j i +1$
\end{itemize}
\[
P_{ij} = \begin{cases}
  \frac{50 -i}{ 100 }   & , \quad  j = i+1 \\
   \frac{50+i}{100}   & , j = i-1 \\
   0  & , \text{otherwise}.
\end{cases}
\] 

% Visualization
% \begin{figure}[ht]
%     \centering
%     \incfig{vizzz}
%     \caption{vizzz}
%     \label{fig:vizzz}
% \end{figure}

\textbf{Motivation}  
\begin{definition}
  For a markov chain, a state $i$ sich that $P_{ij} = 0 \forall j\neq i$  is called \textbf{absorbing.} 
\end{definition}
 \begin{tcolorbox}
   \textbf{Example.} Let $\left\{ X_{n} \right\}$ be a Markov chain woth transition probability matrix \[
   \mathbf{P} = \begin{pmatrix}
   1  &  0  &  0 \\
   \alpha   &  \beta  &  \gamma  \\
   0  & 0  & 1
   \end{pmatrix} 
   \] 
   where $\alpha , \beta , \gamma > 0$ and $\beta = 1- \alpha -\gamma $. Assume $x_{0} = 1$
   \begin{enumerate}
     \item  What is the expected time until absortion ?
     \item What is the probability to be absorbed in state $0$ ?
   \end{enumerate}
 \end{tcolorbox}
 
\section{References}%
\label{sec:references}



\bibliographystyle{plain}
\bibliography{references}
\end{document}

