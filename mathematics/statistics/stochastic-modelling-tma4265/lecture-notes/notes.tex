\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Stochastic Modelling}
\author{isakhammer }
\date{2020}

%%%% DEPENDENCIES v1.3 %%%%%%

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{mathtools}
%\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{todonotes}
\usepackage{esint}
\usepackage{float}


\usepackage{hyperref} 
\hypersetup{
  colorlinks=true, %set true if you want colored links
  linktoc=all,     %set to all if you want both sections and subsections linked
  linkcolor=blue,  %choose some color if you want links to stand out
} 
\hypersetup{linktocpage}


% inscape-figures
\usepackage{import}
\usepackage{pdfpages}
\usepackage{transparent}
\usepackage{xcolor}
\newcommand{\incfig}[2][1]{%
\def\svgwidth{#1\columnwidth}
\import{./figures/}{#2.pdf_tex} } \pdfsuppresswarningpagegroup=1

% Box environment
\usepackage{tcolorbox}
\usepackage{mdframed}
\newmdtheoremenv{definition}{Definition}[section]
\newmdtheoremenv{theorem}{Theorem}[section]
\newmdtheoremenv{lemma}{Lemma}[section]

% \DeclareMathOperator{\span}{span}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
%\newtheorem{example}{Example}

\newcommand{\newpara}
  {
  \vskip 0.4cm
  }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{document}
\maketitle
\tableofcontents
\newpage

\newpage
\section{Lecture 1}%
\label{sec:lecture_1}

\subsection{Practical Information}%
\label{sub:practical_information}

Two projects 
\begin{itemize}
  \item The projects count $20 \%$ and exam $80 \%$.
  \item Must be done with two people.
  \item If you want to do statistics is it worth learning $R$.
\end{itemize}

\textbf{Course Overview} 
\begin{itemize}
  \item Markov chains for discret time and discrete outcome.
    \begin{itemize}
      \item Set of states and discrete time points.
      \item Transition between states
      \item Future depends on the present, but not the past.
    \end{itemize}
  \item Continious time Markoc chains. (continious time and discrete toutcome.
  \item Brownian motion and Gaussian processes (continionus time and continious outcome.)
\end{itemize}


\subsection{Mathematical description}%
\label{sub:mathematical_description}
 \begin{definition}
   A \textbf{stochastic process} $\{ x\left( t \right), t \in T\} $ is a family of random variables, where $T$ is a set of indicies, and $X\left( t \right)$ is a random variable for each value of $t$.
 \end{definition}

\subsection{Recall from Statistics Course}%
\label{sub:recall_from_statistics_course}

A random experiment is perfomed the outcome of the experiment is random.
\begin{itemize}
  \item THe set of possible outcomes is the \textbf{sample space}  $\omega $ 
    \begin{itemize}
      \item An \textbf{event}  $A \subset \omega $  if the outcome is contained in $A$
      \item The \textbf{complement}  of an event $A$ is  $A^{c} = \omega  \setminus A$ 
      \item The \textbf{null event} $\emptyset$ is the empty set $\emptyset = \omega \setminus \omega $ 
    \end{itemize}
\end{itemize}

\subsubsection{Combining Event}%
\label{ssub:combining_event}

Let $A$ and B be events 
\begin{itemize}
  \item The \textbf{union} $A \cup  B$ is the event that at least one of $A$ and $B$ occur.
  \item the \textbf{intersection}  $A \cap B$ is the event that both $A$ and $B$ occur.
\end{itemize}

The events $A_{1}, A_{2}, \ldots$ are called disjoint (or \textbf{mutually exclusive} ) if $A_{i} \cap A_{j} = \emptyset$ for $i \neq j$

\subsubsection{Probability}%
\label{ssub:probability}

$Pr$ is called a probability on $\omega $ if 

\begin{itemize}
  \item Pr $\{ \omega \} = 1  $ 
  \item $0 \le P\left\{ A \right\} \le 1$ for all events $A$ 
  \item For $A_{1}, A_{2} , \ldots$ that are mutually exclusive \[
  P \left\{ \bigcup_{i = 1}^{\infty}A_{i}  \right\} = \sum_{i=1}^{\infty} P \left\{ A_{i} \right\}
  \] 
\end{itemize}
We call $P\left\{ A \right\}$ the probability of $A$.


\subsubsection{Law of total probability}%
\label{ssub:law_of_total_probability}

Let $A_{1}, A_{2}, \ldots$ be a partition of $\omega $ ie 
\begin{itemize}
  \item $\omega  = \bigcup_{i=1}^{\infty} A_{i}$
  \item $A_{1}, A_{2}, A_{3}, \ldots$ are mutually exclusive.
\end{itemize}

Then for any event $B$ \[
  P\left\{ B \right\} = \sum_{i=1}^{\infty} P\left\{ B \cap A_{i} \right\}
\] 

\textbf{This concept is very important.} 

\subsubsection{Independence}%
\label{ssub:independence_2s}
Event $A$ and $B$ are independent of \[
P\left\{ A\cap B \right\} = P\left\{ A \right\}P\left\{ B \right\}
\] 
Events $A_{1}, \ldots, A_{n}$ are independent if for any subset \[
P\left\{ \bigcap_{j=1}^{k} A_{i_j} \right\} = \prod_{j=1}^{k} P \left\{ A_{i_j} \right\} 
\] 

In this case $P\left\{ \bigcap_{i = 1}^{n} A_{1} \right\} =  \prod_{i = 1}^{n} P\left\{ A_{i} \right\} $


\newpage
\subsubsection{Random Variables}%
\label{ssub:random_variables}

\begin{definition}
  A \textbf{random variable}  is a real-vaued function on the sample space. Informally:  A random variable is a real valued variable that takes on its value by chance.
\end{definition}


\begin{tcolorbox}
  \textbf{Example.} 
  \begin{itemize}
    \item Throw two dice. $X = \text{sum of the two dice}$
    \item Throw a coin.  $X$ is $1$ for heads and $X$ is $0$ for tails.
  \end{itemize}
\end{tcolorbox}


\subsubsection{Notation for random variables}%
\label{ssub:notation_for_random_variables}

We use 
\begin{itemize}
  \item upper case letters such at $X$, $Y$ and $Z$  to represent random variables.
  \item lower case letters as $x$, $y$, $z$ to denote the real-valued realized value of a the random variable.
\end{itemize}

Expression such as $\left\{ X \le x \right\}$ denators the event that $X$ assumes a valye less than or earl to the real number x.

\subsubsection{Discrete random variables}%
\label{ssub:discrete_random_variables}

The random variable $X$ is \textbf{discrete}  if it has a finite or countablle number of possible outcomes $x_{1}, x_{2}, \ldots$ \par
\begin{itemize}
  \item The \textbf{probability mass function } $p_{x} \left( x \right) $ is given by \[
  p_{x}\left( x \right) = P \left\{ X = x \right\}
  \] and satisfies \[
  \sum_{i=1}^{\infty} p_{x}\left( x_{i} \right) = 1 \quad  \text{and} \quad  0\le p_{x} \left( x_{i} \right) \le  1 
  \] 
\item The \textbf{cumulative distribution function} (CDF) a of $X$ can be written \[
F_{x}\left( x \right) = P\left\{ X \le x \right\} = \sum_{i: x_{i} \le x}^{} p_{x}\left( x_{i} \right) 
\]  
\end{itemize}

\subsubsection{CFD}%
\label{ssub:cfd} 

The CDF of $X$ may also be called the \textbf{distrobution function}  of $X$ \par 
Let $F_{x}\left( x \right)$ be the CDF of $X$, then 
\begin{itemize}
  \item $F_{x}\left( x \right)$ is monetonaly increasing.
  \item $F_{x}$ is a stepfunction, which is a pieace-wise constant with jumps at $x_{i}.$
  \item $\lim_{x \to \infty} F_{x}\left( x \right) = 1$
  \item $\lim_{x \to - \infty} F_{x}\left( x \right) = 0$
\end{itemize}


\subsubsection{Continious random vairbales}%
\label{ssub:continious_random_vairbales}
 A \textbf{continious} random variables takes value o a continious scale.
 \begin{itemize}
   \item The CDF, $F_{x}\left( x \right) = P \left( X \le x \right)$ is continious.
   \item The \textbf{probability density function} (PDF) $f_{x}\left( x \right) = F_{x}' \left( x \right)$ can be used to calculate probablities \[
   \begin{split}
     Pr \left\{ a < X < b \right\} &=  Pr \left\{ a \le X < b \right\} = Pr\left\{ a < X \le b \right\} \\
     &=  Pr\left\{ a \le X \le b \right\} = \int_{a}^{b}  f_{x}\left( x \right)dx   
   \end{split} 
   \] 
 \end{itemize}


 \subsubsection{Important properties}%
 \label{ssub:important_properties}

 \begin{itemize}
   \item CDF:
     \begin{itemize}
       \item Monotonely increaing
       \item continious
        \item $\lim_{x \to \infty} F_{x} = 1$ and $\lim_{x \to - \infty} F_{x}\left( x \right) = 0$
     \end{itemize}
   \item PDF
     \begin{itemize}
       \item $f_{x}\left( x \right) \ge 0$ for $x \in\mathbb{R} $
       \item $\int_{-\infty}^{\infty} f_{x}\left( x \right)dx = 1$
     \end{itemize}
 \end{itemize}


\subsubsection{Expectation}%
\label{ssub:expectation}

Let $g: \mathbb{R}  \to \mathbb{R} $ be a function and $X$ be a random variable.
\begin{itemize}
  \item If $X$ is discrete, the expected value of $g\left( X \right) $ is \[
  E\left[ g\left( X \right) \right] =  \sum_{x: p_{x}\left( x \right)> 0}^{} g\left( x \right) p_{x}\left( x \right)  
  \] 
\item If $X$ is continous, the expected value of $g\left( X \right) $ is  \[
E\left[ g\left( X \right) \right] = \int_{-\infty}^{\infty} g\left( x \right)f_{x}\left( x \right) dx 
\] 
\end{itemize}

\subsubsection{Variance}%
\label{ssub:variance}

The variance of the random variable $X$ is \[
  Var\left[ X \right] =  E \left[( X - E\left[ X \right])^{2} \right] =  E\left[ X^2 \right] - E\left[ X \right]^2 
\] 
Important properties of expectation and variance.
\begin{itemize}
  \item Expectations is linear \[
  E\left[ aX + bY +c \right] = aE\left[ X \right] + bE\left[ Y \right] + c.
  \] 
\item Variance scales quadratically and is invaraient to the addition of constants \[
Var\left[ aX + b \right] = a^2 Var \left[ X \right] 
\] 
\item fir independent stochastic variables.\[
    Var \left[ X + Y \right] = Var \left[ X \right] + Var\left[ Y \right]
\] 
\end{itemize}

\subsubsection{Joint CDF}%
\label{ssub:joint_cdf}

If $\left( X,Y \right)$ is a pair for random variables, their \textbf{joint comulative distribution function } is given by \[
F_{X,Y} = F\left( x,y \right) =  Pr\left\{ X \le x \cap Y \le y \right\}
\]. 
\subsubsection{Joint distrubution for discrete random variables}%
\label{ssub:joint_distrobution_for_discrete_random_variables}
If $X$ and $Y$  are discrete, the \textbf{joint probability mass function } $ p_{x,y} = Pr\left\{ X = x, Y =y \right\} $. can be used to compute probabilities \[
Pr\left\{ a < X < b, c < Y \le d \right\} =  \sum_{a < x \le b}^{}  \sum_{c < y \le d}^{} p_{X,Y}   \left( x,y \right)
\] 

\subsubsection{Joint distrubution for continous random variables}%
\label{ssub:joint_distrobution_for_continous_random_variables}

If $X$ and $Y$ are continious the \textbf{joint probability density function}  \[
.f_{X,Y} \left( x,y \right) = f\left( x,y \right) = \frac{\partial ^2}{\partial x \partial y } F\left( x,y \right)   
\]  can be used to compute probabilities \[
Pr\left\{ a < X \le b,  \quad  c < Y \le d  \right\} = \int_{a}^{b} \int_{c}^{d} f\left( x,y \right)dxdy    
\] 

\subsubsection{Independence}%
\label{ssub:independence_3}

The random variables $X$ and Y are independent if \[
Pr\left\{ X \le a , Y \le b \right\} =  Pr\left\{ X \le a \right\} \cdot  Pr\left\{ Y \le b \right\}, \quad  \forall a,b \in  \mathbb{R}  
\] 
In terms of CDFs:  $F_{X,Y}(a,b ) =  F_{X}\left( a \right)\cdot F_{Y}\left( b \right) \quad  \forall a,b \in \mathbb{R}  $
\par
Thus we have 
\begin{itemize}
  \item $p_{X,Y} \left( x,y \right) = p_{X}\left( x \right) \cdot  p_{Y}\left( Y \right)$ for discrete random variables
  \item $f_{X,Y}\left( x,y \right) = f_{X}\left( x \right) \cdot  f_{Y}\left( Y \right)$ for continuous random variables.
\end{itemize}






 
 




\newpage
\section{Lecture 3}%
\label{sec:lecture_3}

\subsection{Randoms sum}%
\label{sub:randoms_sum}

Building on the hunter example from last week. we can more generally consider random sums \[
  X = \begin{cases}
    0,  &  \quad  N = 0 \\
    \zeta_{1} + \zeta _{2} + \ldots + \zeta_N , \quad  N >0  
  \end{cases}
\] 
where 
\begin{itemize}
  \item $N$ is a discrete random variable with values $0,1, \ldots$ 
  \item $\zeta _{1}, \zeta _{2}, \ldots $ are independent random variables
  \item $N$ is independent of $\zeta _{1}, \zeta _{2} + \ldots + \zeta _{N}$ 
  \item \textbf{Notation}  $X = \sum_{i=1}^{N} \zeta _{i} = \zeta _{1} + \zeta _{2} + \ldots + \zeta _{N}$ 
\end{itemize}

\begin{tcolorbox}
  \textbf{Example.} 
  \begin{enumerate}
    \item Insurance company \[
    N: \text{ Number of claims.} 
    \] 
  \[
    \zeta _{1} , \zeta _{2} , \ldots \quad  : \quad \text{Sizes of the claims} 
  \] 

  Total liabilility: \[
  X = \zeta _{1}+ \zeta _{2} + \ldots + \zeta _{N}
  \] 
\item  Be careful! \[
    \begin{split}
      \overbrace{E\left[ \sum_{i=1}^{N} \zeta _{i} \right]}^{\neq \sum_{i=1}^{N} E\left[ \zeta _{i} \right]}   & = E\left[ E\left[ \sum_{i=1}^{N} \zeta _{i}  \mid N \right] \right]\\
&= E\left[ \sum_{i=1}^{N} E\left[ \zeta _{i}  \mid  N \right] \right] 
    \end{split} 
\] 
  \end{enumerate}
\end{tcolorbox}

\subsection{Self Study}%
\label{sub:self_study}

Section 2.2, 2.3, 2.4

\subsection{Stochastic process in descrete time}%
\label{sub:stochastic_process_in_descrete_time}
\begin{definition}
  A \textbf{discrete-time stochastic process}  is a family of random variables $\left[ X_{t} : t \in  T \right]$ where $T$ is discrete.
  \begin{itemize}
    \item We use $T = \left\{ 0,1,2,.. \right\}$ and write $X_{n}$ instead of $X_{t}$
    \item  we call $X_{n}$ the \textbf{state}  at time $n =  0,1,2,3, \ldots$
    \item We call the set of all possible states the \textbf{state space} 
  \end{itemize}
\end{definition}

\begin{table}[htpb]
  \centering
  \caption{Table for example}
  \label{tab:label}
  \begin{tabular}{l|cccc}
    Day & $n =0$ & $n=1$ & $n=2$ & \ldots \\ 
    Random Variable  & $X_{0} $ & $X_{1}$ & $X_{2}$ & \ldots \\
    Realization  1& $x_{0} = 0$ & $x_{1} =1$ &  $x_{2} = 1 $ & \ldots \\
    Realization 2 & $x_{0} = 1$ & $x_{1} =1$ &  $x_{2} = 1 $ & \ldots \\
  \end{tabular}
\end{table}
\begin{tcolorbox}
  \textbf{Example.}  \[
  X_{n} = \begin{cases}
    1 ,  &  \quad \text{if it rains on day } n \\
    0,   &  \quad     \text{no rain on day } n
  \end{cases}
  \] 
  State space $= \left\{ 0,1 \right\}$
  \par
  \textbf{We have a problem.} Need \[
  Pr \left \{ X_{n} = x_{n}  \mid  X_{n-1} = x_{n} , X_{n-2} = x_{n-2}, \ldots, X_{0} = x_{0} \right \}.
  \]    for all $n = 0,1,2,\ldots$

\end{tcolorbox}

\subsection{Markov chain}%
\label{sub:markov_chain}


\begin{definition}[Discrete time Markov Chain]
  A \textbf{ Discrete time markoc chain}  is a discrete time stochastic process $\left\{ X_{n} : n = 0,1,\ldots \right\}$ that statisfied the \textbf{markov property}  such that \[
  \begin{split}
       & Pr \left \{ X_{n-1} = j  \mid  X_{n} = i ,    X_{n-1} = i_{n-1} , \ldots, X_{0} = i_{0} \right \}  \\
    &=  Pr \left \{ X_{n+1} = j  \mid  X_{n} = i \right \}  
  \end{split} 
  \] 
  for $n = 0,1,2,3, \ldots$ and for all states $i$ and $j$
\end{definition}

\begin{definition}[One-step transition probabilities]
  We can define it  as 
  \begin{itemize}
    \item For a discrete Markov chain $\left\{ X_{n}: n= 0,1,2, \ldots \right\}$ we call $P_{ij}^{n, n+1} = Pr \left \{ X_{n+1} = j , X_{n} =i \right \} $ the \textbf{one step trainsition probabilities} . 
    \item We will assume \textbf{stationary transition probabilities} , i.e that \[
    P_{ij}^{n, n+1} = P_{ij}
    \]   for $n = 0,1,2, \ldots$ and all states $i $ and $j$ . 
  \end{itemize}
\end{definition}

Some of the properties 
\begin{enumerate}
  \item "You will always go somewhere" \[
  \sum_{j}^{}  P_{ij} = 1 \quad  \forall i 
  \] 
\item The markov chain can be described as follows. \[
    \begin{split}
  & Pr \left \{ X_{0} = i_{0} , X_{1} = i_{1}, \ldots, X_{n} = i_{n} \right \}   \\
 &=  Pr \left \{ X_{0} = i_{0}  \right \}   Pr \left \{ X_{1} = i_{1}  \mid  X_{0} = i_{0} \right \}   \ldots \\
     & \quad \quad    Pr \left \{ X_{n} = x_{n}  \mid  X_{n-1} = i_{n-1} \ldots X_{0} = i_{0} \right \}  \\
  &  \quad \vdots \quad     \text{Markov step} \\
 &=  Pr \left \{ X_{0} = i_{0}  \right \}  \cdot  Pr \left \{ X_{1} = i_{1}  \mid X_{0} = i_{0} \right \} \ldots \\
  & \quad \quad    Pr \left \{ X_{n} = x_{n}  \mid  X_{n-1} = i_{n-1} \right \}   \\
 &=  Pr \left \{ X_{0} = i_{0}  \right \} P_{i_{0}, i_{1}} \cdot  P_{i_{1}, i_{2}} \ldots P_{i_{n-1}, i_{n}}
    \end{split} 
\] 
Which is a major simplification.
\end{enumerate}

\begin{definition}[Transition Probability Matrix] \quad
  For a discrete time markov-chain with state space $\left \{ 0,1, \ldots, N \right \}$ we call
  \[
  \mathbf{P} = \begin{bmatrix} 
    P_{00} & \ldots & P_{0N} \\
    P_{10}  & \ldots \\
    \vdots  &   &  \ddots \\
    P_{N0} & \ldots & P_{NN} 
  \end{bmatrix} 
  \] 
  Is the transition matrix.
  For statespace $\left\{ 0,1,2, \ldots \right\}$ we envision an infinitely sized matrix.
\end{definition}

 \begin{tcolorbox}
   \textbf{Example.} 
   \begin{itemize}
     \item Markoc chain : $\left\{ X_{n} : n = 0,1,2,\ldots \right\}$
     \item State space  $= \left\{ 0,1 \right\}$
     \item Transition Matrix \[
     \mathbf{P} = \begin{bmatrix} 
     0.9  &  0.1 \\
     0.6  &  0.4
     \end{bmatrix} 
     \] 
   \end{itemize}
   We can compute \[
     \begin{split}
        Pr \left \{ X_{3} = 1  \mid  X_{2} = 0 \right \} &=  p_{01} \\
        &= 0.1   \\
        Pr \left \{ X_{10} = 0  \mid  X_{9} = 1 \right \} &=  P_{10} \\
        &= 0.6  \\
     \end{split} 
   \] 
 \end{tcolorbox}

\begin{definition}[Transition Diagram]
  Let $\left\{ X_{n}: n = 0,1, \ldots \right\}$ be a discrete time Markov chain.  A \textbf{state trasnistion diagram} visualizes the transition probabilities as a weighted directed graph where the nodes are the states and the edges are the possible transitions marked with the transistion probabilities.
\end{definition}

\begin{tcolorbox}
  \textbf{Example.} State space $= \left\{ 0,1,2 \right\}$ and \[
  P = \begin{bmatrix} 
  0.95  & 0.05 & 9 \\
  0  & 0.9  &  0.1 \\
  0.01  &  0  &  0.99
  \end{bmatrix} 
  \] 
  Transisition diagram 
  \begin{tcolorbox}
    Nice figure of the diagram
  \end{tcolorbox}
\end{tcolorbox}

 \subsection{Doing n transitions.}%
 \label{sub:doing_n_transitions_}

 \begin{theorem}
   For a Markoc chain $\left\{ X_{n}: n= 0,1, \ldots \right\}$ and any $m\ge 0$ we have \[
     Pr \left \{ X_{m-n} = j  \mid X_{m} = i  \right \}  = P _{ij}^{(n)} =  \sum_{k=0}^{\infty}  P _{ik} P_{kj}^{(n-1)} ,  \quad  n>0 
   \] 
   where we define \[
   P_{ij}^{(0)} = \begin{cases}
     1 , \quad  i= j \\
     0, i \neq j 
   \end{cases}
   \] 
 \end{theorem}

 \begin{proof}
   Set $m = 0$ then is \[
   \begin{split}
     P_{ij }^{(n+1)}   & = Pr \left \{ X_{n+1} = j  \mid  X_{0} = i \right \}   \\
     &= \sum_{k}^{}  Pr \left \{ X_{n+1} = j, X_{1} = k  \mid  X_{0} = i \right \}   \\
     &=  \sum_{k}^{} Pr \left \{ X_{n+1} = j  \mid  X_{1} = k, X_{0} = i \right \} \cdot Pr \left \{ X_{1} = k  \mid  X_{0} = i \right \}   \\
     &= \sum_{k}^{} P_{kj}^{(h)} \cdot P_{ik}  = \sum_{k}^{}  P_{ik} P_{kj}^{(h)}
   \end{split} 
   \] 
 \end{proof}
 
 \begin{tcolorbox}
   \textbf{Example.} $\left\{ X_{n} : n= 0,1,2, \ldots \right\}$ is a markoc chain and \[
   P = \begin{bmatrix} 
   0.1  &  0.9 \\
   0.6  &  0.4 
   \end{bmatrix} 
   \] 
   Find $P_{01}^{(4)}$ .
   \textbf{Solution}. \[
   P^2 = \begin{bmatrix} 
   0.55  &  0.45 \\
   0.30  &  0.70
   \end{bmatrix} 
   \] 
   So by doing matrix multiplication and we end up with \[
   P^{4} = P^{2} \cdot  P^{2} = \begin{bmatrix} 
   0.4375  &  0.5625 \\
   0.3750  &  0.6250
   \end{bmatrix} 
   \] 
   Which therefore ends up with the answer \[
   P_{01}^{(4)} = 0.5625
   \] 
 \end{tcolorbox}
 




\newpage

\section{Lecture 4}%
\label{sec:lecture_4}

 \subsection{Introduction to first step analysis}%
 \label{sub:introduction_to_first_step_analysis}

 \textbf{Input} 
 \begin{itemize}
   \item $i_{0}$ : starting state
    \item $P$ : transition probability matrix
    \item T: number of time steps
 \end{itemize}
 \textbf{Algorithm} 
 \begin{enumerate}
   \item Set $x_{0} = i_{0}$
   \item for $n=1 \ldots T$
   \item $\quad   $ Simulate $x_{n}$ from $X_{n}  \mid  X_{n-1} = x_{n-1}$
   \item end
 \end{enumerate}
 
 \textbf{output} : One realization $x_{0}, x_{1} , \ldots, x_{T}$ 
 
 \begin{tcolorbox}
   \textbf{Example.} 
   \[
   P = \begin{pmatrix}
   0.95  &  0.05  &  0 \\
   0  &  0.90  &  0.10 \\
   0.01  &  0  &  0.99
   \end{pmatrix} 
   \] 
   Let $x_{0} = 0$
   \begin{enumerate}
     \item $x_{0} = 0$  
     \item 
       \begin{align*}
       Pr \left \{ X_{1} = 0 | X_{0} = 0 \right \} = &  P_{00} = 0.95  \\
       Pr \left \{ X_{1}  \mid  X_{0}  \right \}  &=  P_{01} = 0.05 \\
       Pr \left \{ X_{1}  \mid  X_{0} = 0 \right \}  &=  P_{02} = 0 \\
       .\end{align*}
       Assume we get $x_{1} = 1$
     \item States 
       \begin{itemize}
         \item \[
             \begin{split}
         0: P_{10}  &=  0 \\
         1: P_{11 } &=  0.90 \\
         2: P_{12} &=  0.10 \\
         \vdots  \\
             \end{split} 
         \] 
       \end{itemize}
   \end{enumerate}
 \end{tcolorbox}

 \begin{tcolorbox}
   General notes on simulation 
   \begin{itemize}
     \item
   $Pr \left \{ A  \right \} \approx \frac{\text{# times A occure}}{ \text{ Simulations}}  $
 \item $E\left[ X \right] \approx \frac{1}{N}  \sum_{i=1}^{B}  x_{i}$
   \end{itemize}
 \end{tcolorbox}

   \textbf{Example.} We have $N=100$ divided into two containers labelled $A$ and $ b$. At each time $n$, one ball is selected at random and moved to the container. Let $Y_{n}$ denote the number of balls in container $A$ at time $n$, and define $X_{n} = Y_{n} -50$. Find the transition probabilities and simulate and plot one realization of \[
   \left\{ X_{n}: n  = 0,1, \ldots, 500 \right\}
   \] 

   \textbf{Answer} 
 
\begin{figure}[ht]
    \centering
    \incfig{balls}
    \caption{balls}
    \label{fig:balls}
\end{figure}

\begin{itemize}
  \item Only move One ball
  \item Can move only from $i$ to $ j = i-1$ or  $j i +1$
\end{itemize}
\[
P_{ij} = \begin{cases}
  \frac{50 -i}{ 100 }   & , \quad  j = i+1 \\
   \frac{50+i}{100}   & , j = i-1 \\
   0  & , \text{otherwise}.
\end{cases}
\] 

% Visualization
% \begin{figure}[ht]
%     \centering
%     \incfig{vizzz}
%     \caption{vizzz}
%     \label{fig:vizzz}
% \end{figure}

\textbf{Motivation}  
\begin{definition}
  For a markov chain, a state $i$ sich that $P_{ij} = 0 \forall j\neq i$  is called \textbf{absorbing.} 
\end{definition}
 \begin{tcolorbox}
   \textbf{Example.} Let $\left\{ X_{n} \right\}$ be a Markov chain woth transition probability matrix \[
   \mathbf{P} = \begin{pmatrix}
   1  &  0  &  0 \\
   \alpha   &  \beta  &  \gamma  \\
   0  & 0  & 1
   \end{pmatrix} 
   \] 
   where $\alpha , \beta , \gamma > 0$ and $\beta = 1- \alpha -\gamma $. Assume $x_{0} = 1$
   \begin{enumerate}
     \item  What is the expected time until absortion ?
     \item What is the probability to be absorbed in state $0$ ?
   \end{enumerate}

   \textbf{Realization} . \[
   \overbrace{1,1,1,1,1,2}^{4 \text{ steps to absorption}} ,2,2 \ldots
   \] 
   \textbf{Mathematically} 
   
   \newpara
   Let $T = \min_{} \left\{ n \ge 0 : X_{n} = 0 \quad \text{or} \quad  X_{n} = 2   \right\}$. Then is \[
   \begin{split}
     Q1:  &  \quad E\left[ T  \mid X_{0} = 1 \right]  \\
     Q2:  &  \quad  Pr \left \{ X_{T} = 0  \mid  X_{0} = 1 \right \}  
   \end{split} 
   \] 
   The idea of first step analysis is to define 
   \begin{itemize}
     \item $T^{(n) } = \min_{}  \left\{ n \ge :: X_{m\times n } = 0 \quad \text{or} \quad X _{m+b} =2   \right\}$
     \item $T = T^{(0)}$
     \item $v^{(m)}_{i} = E\left[ T^{(m) }  \mid  X_{m}  = i \right]$
     \item $v_{i} = v^{(0)} _{i}$
   \end{itemize}


 \end{tcolorbox}
   \begin{table}[htpb]
     \centering
     \caption{Let $m$ be timesteps}
     \label{tab:label2}
     \begin{tabular}{l|ccccc}
     $m$   & $0$  & $2$   &3  &4  & 5  \\
     $v^{(m)}_{0}$   &  $0$ & $0$   & $0$  & $0$  & $0$  \\
     $v^{(m)}_{1}$   & $v_{1}$  &  $v_{1}$& $v_{1}$ & $v_{1}$ &$v_{1}$  \\
     $v^{(m)}_{2}$   & $0$ & $0$  &  $0$& $0$  & $0$  \\
     \end{tabular}
   \end{table}

   \textbf{First step analysis for Q1} 
   \[
     \begin{split}
   v_{i}  & = \sum_{k=0}^{2}  Pr \left \{ X_{1} = k  \mid  X_{0} = i \right \}  \left( 1 + v_{k} \right) \\
&= \sum_{k=0}^{2}  P_{ik} \left( 1+ v_{k} \right) = \sum_{k=0}^{2} P_{ik} v_{k} +1 \quad  \text{which is true for } \quad i = 0,1,2   \\
     \end{split} 
   \] 
   Which is reduced to linear algebra. Solving it by \[
   \begin{split}
     v_{0} &=  v_{2} = 0 \\
     \implies  v_{1} &= \alpha  v_{0} + \beta v_{1} + \gamma v_{2} + 1 \\
     \implies  v_{1} &=  \frac{1}{ 1- \beta } \quad  \text{[Q1]}  \\
   \end{split} 
   \] 
   \begin{tcolorbox}
     $P_{ij} \implies  i = \text{row} , \quad j = \text{column} $
   \end{tcolorbox}
   First step analyis and let \[
     \begin{split}
   u_{i}  & = Pr \left \{ X_{T} = 0  \mid  X_{0} = i \right \}  \\
     &  \downarrow \\
   u_{i} &= \sum_{k=0}^{2}  P_{ik} u_{k}, \quad i = 0,1,2   \\
     \end{split} 
   \] 
     \begin{itemize}
       \item Easy: $u_{0} = 1, u_{2} =0$
       \item Harder: $u_{1} = \alpha  u_{0} + \beta u_{1} + \gamma u_{2}$ such that \[
           u_{1} = \alpha \frac{1}{1- \beta }  = \frac{\alpha }{\alpha  - \beta }  \quad \text{[Q2]} 
       \] 
     \end{itemize}
   
     \begin{tcolorbox}
       \textbf{Example.} let $\left[ X_{n} \right]$ be a markov chain with transition matrix \[
       \mathbf{P} = \begin{pmatrix}
       1  &  0 &  0  &  0 \\
       0.4  &  0.3  &  0.2  &  0.1 \\
       0.1  &  0.3  &  0.3 &  0.3 \\
       0  &  0 &  0 &  0 
       \end{pmatrix} 
       \] 
       The starting state is $x_{0} = 1$. Calculate the probability to be absorbed in the state $D$.
       \begin{enumerate}
         \item Define $u_{i} = Pr \left \{ \text{absorbed in state 0}  \mid  X _{0} = i \right \} $ for $i = 0,1,2,3$ 
         \item Get the easy ones out of the way. In this case $u_{0} = 1$ and $u_{3} = 0$
         \item 
           \[
           \begin{split}
            u_{1}  &  = P_{10} u_{0} + P_{11} u_{1} + P_{12} u_{2} + P_{13}u_{3}  \\
            &= 0.4 + 0.3 u_{1} + 0.2 u_{2}  \\
            u_{2} &= P_{20} u_{0} + P_{21} u_{1} + P_{22} u_{2} + P_{23} u_{3} \\
            &= 0.1 + 0.3 u_{1} + 0.3 u _{2} \\
           \end{split} 
           \] 
         \item Solve for $u_{1}$ and $u_{2}$
       \end{enumerate}
     \end{tcolorbox}
 
\newpage
\section{Lecture 5}%
\label{sec:lecture_5}

\textbf{Example.} Let $P$ be the matrix \[
P = \begin{bmatrix} 
1  & 0  &  0 &  0 \\
0.4  &  0.3  &  0.2  &  0.1 \\
0.1   &  0.3  &  0.3  &  0.3 \\
0  &  0  &  0 &  0
\end{bmatrix} 
\] 
With starting state $x_{0} = 1$

\begin{enumerate}
  \item Define $T  = \min_{ n\ge 0 : X_{n} = 0 \quad X_{n} = 3 } $ and $v_{i} = E \left[ T  \mid  X_{0} = i \right] $ for $i  = 0,1,2,3 $
  \item Set $v_{0} = v_{3} = 0$
  \item \[
  v_{1} = P_{10} v_{0} + P_{11} v_{1} + P_{12} v_{2} + P_{13} v_{3}  = 0.3 v_{1} + 0.2_{v2} +1
  \] 
  and \[
  v_{2} = P_{20 } v_{0}  + P_{21} v_{1} + P_{22}v_{2} + P_{23 } v_{3} + 1 = 0.3 v_{1} + 0.3 v_{2} + 1
  \] 
\item Solve the equations and end up with \[
v_{1} = \frac{90}{43} \quad \text{and} \quad v_{2} \frac{100}{43}  
\] 

\end{enumerate}


\begin{theorem}
  Let $\left\{ X_{n} \right\}$ be a discrete time Markov chain with state space $S = \left\{ 0, 1, \ldots , N \right\} $ and transition probability matrx $\mathbf{P}$. Let $A \subset S$ be the set of absorbing state. Then
  \begin{enumerate}
    \item If $v_{i}$ is the expected time to absorption conditional on $X_{0} = i$ then \[
    \begin{split}
      v_{i}  & = 0, \quad  i \in  A   \\
      v_{i} &=  1+ \sum_{ i \in  \mathbb{R} }^{} P_{ik} v_{k} \quad i \in  A^{c}  \\
    \end{split} 
    \] 
  \end{enumerate}
\end{theorem}


\textbf{Example.} 
A gambler has $10 \$$ and bets  $1\$$  If he wins the round, his fortune increases $ 1 \$$.  The probability of winning each round is  $ 0  < p < 1$ and the probability of losing each round is $q = 1 - p$. The gambler will continue gambling until his fortine is \$ $N$ or $0 \$$ where  $N > 10$. What is the probability the gambler will be ruined. 

\begin{enumerate}
  \item Extract the essential stuff. \[
      \begin{split}
        X_{n} &=  \text{Fortune at time} \quad  n, \quad n = 0,1,2,\ldots   \\
        \text{State space } &=  \left\{ 0,1, \ldots, N \right\} \\
        \text{Target: } u_{k}   & = Pr \left \{ \text{Absorption in state 0}  \mid X_{0} = k \right \}, \quad  k = 0,1, \ldots, N \\
      \end{split} 
  \] 
\item  Visualize the transitions.  Insert figure of tranistions.
   \item Make the eprobability matrix. The rows are "to" and the columns are "1"
     \[
     P = \begin{bmatrix} 
     1   &  0 & 0 & 0  & \ldots  &  0 \\
     q  & 0  &   p  & 0  &   \ldots  &  0 \\
     0  &  q  &   0  &  p  &  \ldots   \\
     
     \vdots   &  &   \ddots    \\
      &  &  & q  & 0  &  p \\
     0  &  0  &  \ldots  &   &  & 1
     \end{bmatrix} 
     \] 
   \item Set up the iteration \[
   \begin{split}
     u _{0}  & = 1 , \quad  u_{N} = 0 , \quad  \text{Easy}   \\
     u_{i} &=  P_{i, i.1} u_{i-1} + P _{i,i+1} u_{i+1}  \\
     &= q u_{i-1} + p u_{i+1} , \quad i=1,2, \ldots, N-1  \\
   \end{split} 
   \] 
 \item 
   \begin{enumerate}
     \item  \[
         \begin{split}
           \overbrace{(p + q)}^{ = 1} u_{i}   & = q u_{i -1} + p u_{i+1} \\
    q\left[ u _{ i} - u_{i-1} \right] &=  p \left[ u_{i+1} - u_{i} \right] \\
      &  \downarrow \quad \text{Trick}    \quad \chi _{i} = u_{i} - u_{i-1}   \\
    q \chi _{1} &=  p \chi _{i+1} , \quad  \implies  \chi _{i+1} = \frac{q}{p}  \chi _{i}  \quad  i = 1,2 , \ldots , N  \\
         \end{split} 
     \] 
   \item \[
   \begin{split}
     \chi _{1} + \chi _{2} + \ldots + \chi _{k} &= \left[ u - u_{0} \right] + \left[ u_{2} - u_{1} \right] + \ldots + \left[ u_{k} - u_{k-1} \right] \\
       &  \downarrow  \quad \text{Telescoping sum}  \\
        \chi _{1}\left[ 1 + \frac{q}{p} + \left( \frac{q}{p} \right)^2 + \ldots + \left( \frac{q}{p} \right)^{k-1}  \right]&=  u_{k} -1, \quad k = 1,2,3,\ldots,N  \\
   \end{split} 
   \] 
   For $k = N$ : \[
   \begin{split}
     \chi _{1} &=    \frac{u_{N} -1}{\sum_{k=0}^{N-1} \left( \frac{q}{p} \right)^{k}}  = \frac{-1}{ \sum_{k = 0}^{N-1}  \left( \frac{q}{p} \right)^{k}}   \\
     &= \begin{cases}
       -\frac{1}{N}\quad   & , q = p = \frac{1}{2} \\
       \frac{- \left( 1- \frac{q}{p} \right)}{\left( 1- \left( \frac{q}{p} \right) \right)} \quad   &  q\neq p \\
     \end{cases} \\
   \end{split} 
   \] 
 \item 
   From  the telescoping sum 
   \begin{align*}
     u_{k} &=  1+ \chi _{1} \sum_{i=0}^{k-1} \left( \frac{q}{p} \right)^{i} \\
     &=  \begin{cases}
       1-\frac{1}{N} \cdot k =  \frac{N-k}{N}  ,  &  \quad  p = q = \frac{1}{2}  \\
       1- \frac{1- \left( \frac{q}{p}  \right)^{k}}{ 1- \left( \frac{q}{p} \right)^{N}} = \frac{\left( \frac{q}{p} \right)^{k} - \left( \frac{q}{p} \right) ^{ N}}{1- \left( \frac{q}{p} \right) ^{N}}  ,   & \quad  p\neq q 
     \end{cases}, \quad \text{where} \quad    k = 1,2, \ldots.  \\
   .\end{align*}
   \end{enumerate}
 \item The final step \[
 u_{10} = \begin{cases}
   \frac{N- 10}{ N}  ,  &  \quad  p = q = \frac{1}{2} \\
   \frac{\left( \frac{q}{p} \right)^{10} - \left( \frac{q}{p} \right) ^{N}}{ 1 - \left( \frac{q}{p} \right)^{N}}  , &\quad     q \neq p 
 \end{cases}
 \] 
\end{enumerate}

\begin{remark}
  \begin{itemize}
    \item When $N \to  \infty$ \[
        \begin{split}
    q \ge p \quad   & \implies  \quad  \text{Almost certain you will loose.}   \\
    q < p  & \implies  P\left( \text{ruined} \right) = \left( \frac{q}{p} \right)^{10} \\
        \end{split} 
    \] 
  \end{itemize}
\end{remark}

\subsection{Markov Chain in infinitive time}%
\label{sub:markov_chain_in_infinitive_time}

\begin{definition}
  \textbf{Regular Markov Chain} . Consider a Markov chain $\left\{ X_{n}: \quad  n = 0,1,\ldots  \right\}$ with finite state space $ \left\{ 0,1,2, \ldots \right\}$ and transition matrix $\mathbf{P}$. IF there exists an integer $k >0$ so that all regular elements $\mathbf{P}^{k}$ are strictly positive, we call $\mathbf{P}$ and $\left\{ X_{n} \right\}$ regular. 
\end{definition}

\begin{remark}
   \begin{enumerate}
     \item $P$ is regular means that it exists an $k > 0$ so that $P^{(k)} _{ ij} > 0 \quad  \forall i,j $
     \item  If $P^{(k)} _{ij} \quad  \forall i,j $, then is $P^{(k)} _{ij} > 0 \quad  \forall i,j $ and $K \ge k$ 
   \end{enumerate}
\end{remark}

\newpage
\section{Lecture 2020-09-14}%
\label{sec:lecture_2020_09_14}

Find Stationary distributions

\begin{enumerate}[label=(\roman*)]
  \item $\mathbf{P } = \begin{bmatrix} 
  \frac{1}{2}  &  \frac{1}{2} \\
  \frac{1}{2}  &  \frac{1}{2} 
  \end{bmatrix} 
  $
  
  \newpara
  \begin{itemize}
    \item 
  Positive recurrent, aperiodic  and irreducible. 
\item 
  $\implies $ Limiting distribution: \[
  \mathbf{\pi } = \left( \frac{1}{2}, \frac{1}{2} \right)
  \] 
  \end{itemize}
\item $\mathbf{P} = \begin{bmatrix} 
0  &  1 \\
1  &  0
\end{bmatrix} 
$
\begin{itemize}
  \item Positive recurrent and irreducible.
  \item unique stationary distrobution.
  \item $\mathbf{\pi } = \left( \frac{1}{2} , \frac{1}{2} \right)$
\end{itemize}

\item $\mathbf{P} = \begin{bmatrix} 
1  &  0 \\
0  &  1
\end{bmatrix} 
$

\begin{itemize}
  \item Reducible!
  \item Part 1: \[
      \begin{split}
  \pi _{0}  & = 1 \pi _{0} = 0 \pi _{1} = \pi _{0} \\
  \pi _{1}  & = 0 \pi _{0} + 1 \pi _{1} = \pi _{1} \\
  \implies  \pi _{}  & = 1 - \pi _{1} \\
  \implies  \mathbf{\pi }  & = \left( \pi _{1} , 1 - \pi _{1} \right)
      \end{split} 
  \] 
\item Part 2: 

\newpara
Must have \[
\begin{split}
  \pi _{0} \ge 0 \\
  \pi _{1 } \ge 0 \\
  \implies  \mathbf{\pi } =  \left( \pi _{0} , 1- \pi _{0}  \right)  & , \quad 0\le \pi _{0} \le 1 
\end{split} 
\] 
\end{itemize}
\end{enumerate}

\subsubsection{Section 4.5}%
\label{ssub:section_4_5}
Read it yourself .


\subsection{Why do we care so much about markov chains?}%
\label{sub:why_do_we_care_so_much_about_markov_chains_}

\begin{enumerate}[label=(\roman*)]
  \item Importance goes far beyond statisical modelling of physical phenomena.
  \item In the end of the 80s and start of 90s the computationally power was growing stronger.
  \item We realized that we could sample from difficult distribution  by constructing Makov chains whose stationionary matched desired target distrobution.
  \item The theory we have descussed of the theory developed to show that these methods worked.
\end{enumerate}

\subsection{Continuous Time Markov Chain}%
\label{sub:continuous_time_markov_chain}


\begin{definition}
  The Sstochastic variable $X$  has a \textbf{Poission distribution}  with (mean) parameter $\mu > 0$ if \[
  p\left( x \right) = \frac{\mu ^{x}}{x!}  e^{- \mu }
  \] 
  We write $X \sim Possion (\mu )$
\end{definition}

\begin{remark}
  $X \sim Poission (10)$
   \begin{enumerate}[label=(\roman*)]
    \item $E\left[ X \right] = \mu $
    \item $Var\left[ X \right] = \mu $
    \item $SD\left[ X \right] \sqrt{\mu } $
  \end{enumerate}
\end{remark}

\begin{theorem}
  If $X \sim Possion (\mu ), \quad  Y \sim \left( \chi  \right) $ and $Y $ are independent. 
\end{theorem}

\begin{theorem}
  If $N \sim Possion(\mu )$  and $ M  \mid  N \sim Binomial(N,p)$ then \[
    M \sim Poission(\mu P)
  \]  
\end{theorem}

\begin{remark}
  \begin{enumerate}[label=(\roman*)]
    \item $M = \sum_{k= 1}^{ N}  I_{k}$ , where $ I_{1}, I_{2} , \ldots \sim Bernoulli (p)$ and $I_{1}, I_{2} , \ldots$ and $N$ are independent. 
    \item This is called \textbf{thinning}.  
  \end{enumerate}
\end{remark}

\subsubsection{Section 5.1.2}%
\label{ssub:section_5_1_2}

\begin{definition}
  A \textbf{Possion process}  with rate  \textbf{inensity}  $ \lambda > 0$ is an integet-valued stochastic process $\left\{ X\left( t \right): t \ge 0 \right\}$ 0 for which.  
  \begin{itemize}
    \item For any $n>0$ and any time point $0< t_{0} < t_{1} < \ldots < t_{n}$ the increments \[
    X\left( t_{1}  \right) - X\left( t_{0} \right) , X\left( t_{2} \right) - X\left( t_{1} \right) , \ldots , \ldots X\left( t_{n} \right) - X\left( t_{n-1} \right)
    \] 
    are independent 
  \item For $s\ge 0$ and $t> 0$  \[
      X\left( s+t \right)  - X\left( s \right) \sim Poission(\lambda t)
  \] 
\item $X\left( 0 \right) = 0$
  \end{itemize}
\end{definition}

\begin{remark}
  \begin{itemize}
    \item 1. is called independent increments 
    \item In 2, we have \[
        X\left( s+ \Delta t \right) - X\left( s \right) \sim Possion(\lambda  \Delta t)
    \] 
  \item Illustration 
\begin{figure}[ht]
    \centering
    \incfig{iillustration}
    \caption{iillustration}
    \label{fig:iillustration}
\end{figure}
\item $X\left( t \right) = X\left( t \right) - X\left( 0 \right) \sim Possion(\lambda t)$
  \end{itemize}
\end{remark}
\begin{tcolorbox}
  \textbf{Example.} We assume the arrival of customers to a store follows a Poission process with rate $\lambda  = 4$ customers per hours. The stor opens a 09:00. What is the probability that exactly one customer has arrived by 09:30 and exactly five customers have arrived bu 11:30.
  
  \newpara
  \textbf{Answer.} Let $X\left( t \right) = \text{#arrivals by time $t$}$ For $t\ge 0$ (measured in hours). Then is the question \[
    \begin{split}
  Pr \left \{ X\left( \frac{1}{2} \right) = 1,  &   X\left( \frac{5}{2}  \right) = 5 \right \}  \\
  \downarrow   &  = \text{ Rephrase as incements } \\
               &= Pr \left \{ X\left( \frac{1}{2} \right) - X\left( 0 \right) = 1 , X\left( \frac{5}{2}  \right) - X\left( \frac{1}{2} \right) = 4  \right \}  \\
               \downarrow  &  \text{Independent increments} \\
                           &= Pr  \underbrace{\left \{ X\left( \frac{1}{2} \right)- X\left( 0 \right) = 1 \right \}}_{ Poission(\frac{1}{2} \lambda )}  \cdot  Pr  \overbrace{\left \{X\left( \frac{5}{2} \right) - X\left( \frac{1}{2} \right\}}^{Possion(2 \lambda )}  = 4 \right \}  \\ 
&= \frac{2^{1}}{1!} e^{-2}  \cdot \frac{8^{4}}{4!} e^{-8}   \\
    &= 0.0155 \\
    \end{split} 
  \] 
\end{tcolorbox}
\begin{tcolorbox}
  \textbf{Example.} Assume the arrical of customers to follows an inhomogenous Poission process with rate $\lambda \left( t \right)  =t$ , $t\ge 0 $. Assume the store opens at $09:00$. What is the probability that no-one has arrived at $10:00$.
   
  \newpara
  \textbf{Answer.} \[
    \begin{split}
  X\left( 1 \right) - X\left( 0 \right)  & \sim Poission\overbrace{\left( \int_{0}^{ 1}  t dt  \right)}^{ = \frac{1}{2}}   \\
    \end{split} 
  \]  
\end{tcolorbox}
\newpage
\section{Lecture 08/09/20}%
\label{sec:lecture_08_09_20}

Equivalent classes and classifications of states in Markov chains.


\newpara
Things to check
\begin{itemize}
  \item Understand why regularity fails.
  \item Extend regularity to infinite spaces.
\end{itemize}

\textbf{Example} 
Let $\left\{ X_{n}: 0,1,\ldots,N \right\}$ be a markov chain.

\begin{enumerate}[label=(\alph*)]
  \item It can go from $0\to 0$ and $1\to $ with probabilities $p_{00} = p_{11} = 1$, two seperate markov chains. Realizations : \[
  \begin{split}
      &  0, 0,0,0,0,0,    \ldots\\
      &  1,1,1,1,1,1,  \ldots  \\
    P &= \begin{bmatrix} 
    1  &  0 \\
    0  &  1
    \end{bmatrix}  \implies  P^{n} = \begin{bmatrix} 
    1  &  0 \\
    0  &  1
    \end{bmatrix} 
     \\
  \end{split} 
  \] 
\end{enumerate}

\begin{definition}
  Let $\left\{ X_{n}: 0, 1 , \ldots \right\}$ be a Markov chain with state space $\left\{ 0, 1 , \ldots \right\}$ then is 
  \begin{enumerate}[label=(\roman*)]
    \item State $j$ is \textbf{ accessible}  from state $i $ if $\exists n \ge 0$  so that $P^{(n)} > 0$ 
    \item If states $i$ and $j$ are accessable from each other they are said to \textbf{communcate }  we write $ i \sim j$. If states $i$ and $j$ do not communcate we write $i \not \sim  j$
  \end{enumerate}
\end{definition}

\begin{remark}
  If $ i \not \sim  j$ , then either (or both) 
  \begin{enumerate}[label=(\alph*)]
    \item
    \begin{enumerate}[label=(\roman*)]
    \item $P^{(n)}_{ij} = 0, \quad  \forall n\ge0 $
    \item  $P_{ji} = 0, \quad  \forall n\ge0 $
    \end{enumerate}
  \item Only the graph matters, not the values of the edges.
  \item $P^{(0)}_{ij} = \begin{cases}
    1,  &  i = j \\
    0,  &  i\neq j
  \end{cases}$

  \end{enumerate}
\end{remark}

\begin{theorem}
  Communication is an \textbf{equicalence relation} 
  \begin{enumerate}[label=(\roman*)]
    \item \textbf{reflexive} , $i ~ j$
    \item  \textbf{symmentric }  $i \sim  j \implies  j \sim i$
    \item \textbf{Transitive}  $i \sim j$ and $ j \sim k$ implies $i \sim k$
  \end{enumerate}
  A equivalence relation induces \textbf{equivalence classes} of sets of states that communicate.
\end{theorem}

\begin{proof}
  \begin{enumerate}[label=(\roman*)]
    \item $P ^{\left( 0 \right)} _{ii} = 1 \quad  \implies  i \sim i   $ 
    \item By definition is this true.
    \item 
      \begin{enumerate}[label=(\alph*)]
        \item $ i \sim j$ $\implies  \quad  \exists n\ge 0: P ^{(n)} _{ij} > 0 $ \[
        j \sim  k \implies  \exists m\ge 0 : P^{(m)} _{jk} > 0
        \] 
      \item Chapman-kilogram \[
      P^{(n+m)} _{ik} = \sum_{r=0}^{\infty} P^{(n)}_{ir} P^{(m)}_{rj} \ge P^{(n)}_{ij} P^{(m)}_{jk}
      \] 
      $\implies $ $k$ is accessible from $i$.
    \item Show yourself 
      
      \newpara
      \textbf{$i$ is accessible from $k$} 

      \end{enumerate}
  \end{enumerate}
\end{proof}

\begin{definition}
  A Markov chain is \textbf{irreducible}  if $ \sim $ (communication) induces exactly one equivalent class.  If not, it is called reducible. 
\end{definition}
\begin{definition}
  The \textbf{period}  of state $i$, written as $d\left( i \right)$ is \[
  d\left( i \right) = \text{gcd}\left\{ n \ge 1: P ^{(n)} _{ii} > 0 \right\}
  \] 
  If $P^{(n)} _{ii} = 0$ for all $n\ge 1$, we define $d\left( i \right) = 0$. If $d\left( i \right) = 1$, we call the state $i$ is \textbf{aperiodic.} 
\end{definition}
\begin{theorem}
  if $i \sim j$, then $d\left( i \right) = d\left( j \right)$
\end{theorem}
\begin{remark}
   The period is a property of the equivalence class.
\end{remark}

\textbf{Notation} THe state space may be infinite: $\left\{ 0,1, \ldots \right\}$. We introduce  
\begin{enumerate}[label=(\roman*)]
  \item The probability the first return happend after exactly $n$ steps \[
  f_{ii}^{(n)} = Pr \left \{ X_{n} = i, X_{\mu }\neq i, i  = 1,2, \ldots, n-1  \mid X_{0} = i  \right \}  \quad  n> 0 
  \] 
  We will define $f_{ii}^{(0)} = 0$
\item The probability of returning at some time \[
f_{ii} = \sum_{k= 0}^{\infty}  f_{ii}^{(k)} = \lim_{n \to  \infty}  \sum_{k= 0}^{n} f^{(k)} _{ii} .
\] 
\end{enumerate}
 \begin{remark}
   $f_{ii} < i \leftrightarrow  \text{Positive probability of never returning to $i$}$
 \end{remark}

 \begin{definition}
   State $i$ is \textbf{recurrent}  if the probabilitu of retunging to sate $i$ in a finite number of timesteps is one $f_{ii} = 1$. A state that is not recurrent $f_{ii} < 1$ is called \textbf{transient} .
 \end{definition}

 \begin{theorem}
   A state $i$ is recurrent if and only if \[
   \sum_{n=1}^{\infty} P_{ii}^{(n)} = \infty
   \] 
   Equivalently, state $i$ is transient if and only if \[
   \sum_{n=1}^{\infty}  P^{(n)}_{ii} < \infty
   \] 
 \end{theorem}

 \begin{proof}
   \begin{enumerate}[label=(\roman*)]
     \item  \[
         \begin{split}
\sum_{n =1}^{\infty}  P _{ii}^{(n)}  & = \sum_{n= 1}^{\infty}  E\left[ \mathbb{I} \left\{ X_{n} = j \right\}  \mid  X_{0} = j \right]  \\
       & = E\left[ \sum_{n=1}^{ \infty}  \mathbb{I} \left\{ X_{n} = i  \mid  X_{0} = i \right\} \right] \\
       &= E\left[ M  \mid  X_{0} = i \right] \\
        & M \to  \text{Returns to state.}
         \end{split} 
     \] 
   \item $E \left[ M  \mid  X_{0} = i \right] = \begin{cases}
     f_{ii}\frac{1}{1- f_{ii}}  , &  \quad  f_{ii} < 1 \\
     \infty ,  &  f_{ii} = 1 
   \end{cases}$
   \end{enumerate}
 \end{proof}

\section{References}
\label{sec:references}



\bibliographystyle{plain}
\bibliography{references}
\end{document}

