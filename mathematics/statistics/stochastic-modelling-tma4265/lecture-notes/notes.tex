\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Stochastic Modelling}
\author{isakhammer }
\date{2020}

%%%% DEPENDENCIES v1.1 %%%%%%

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{enumerate}
\usepackage{todonotes}
\usepackage{float}


\usepackage{hyperref} 
\hypersetup{
  colorlinks=true, %set true if you want colored links
  linktoc=all,     %set to all if you want both sections and subsections linked
  linkcolor=blue,  %choose some color if you want links to stand out
} 
\hypersetup{linktocpage}


% inscape-figures
\usepackage{import}
\usepackage{pdfpages}
\usepackage{transparent}
\usepackage{xcolor}
\newcommand{\incfig}[2][1]{%
\def\svgwidth{#1\columnwidth}
\import{./figures/}{#2.pdf_tex} } \pdfsuppresswarningpagegroup=1

% Box environment
\usepackage{tcolorbox}
\usepackage{mdframed}
\newmdtheoremenv{definition}{Definition}[section]
\newmdtheoremenv{theorem}{Theorem}[section]
\newmdtheoremenv{lemma}{Lemma}[section]

% \DeclareMathOperator{\span}{span}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
%\newtheorem{example}{Example}

\newcommand{\newpara}
  {
  \vskip 0.4cm
  }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{document}
\maketitle
\tableofcontents
\newpage

\newpage
\section{Lecture 1}%
\label{sec:lecture_1}

\subsection{Practical Information}%
\label{sub:practical_information}

Two projects 
\begin{itemize}
  \item The projects count $20 \%$ and exam $80 \%$.
  \item Must be done with two people.
  \item If you want to do statistics is it worth learning $R$.
\end{itemize}

\textbf{Course Overview} 
\begin{itemize}
  \item Markov chains for discret time and discrete outcome.
    \begin{itemize}
      \item Set of states and discrete time points.
      \item Transition between states
      \item Future depends on the present, but not the past.
    \end{itemize}
  \item Continious time Markoc chains. (continious time and discrete toutcome.
  \item Brownian motion and Gaussian processes (continionus time and continious outcome.)
\end{itemize}


\subsection{Mathematical description}%
\label{sub:mathematical_description}
 \begin{definition}
   A \textbf{stochastic process} $\{ x\left( t \right), t \in T\} $ is a family of random variables, where $T$ is a set of indicies, and $X\left( t \right)$ is a random variable for each value of $t$.
 \end{definition}

\subsection{Recall from Statistics Course}%
\label{sub:recall_from_statistics_course}

A random experiment is perfomed the outcome of the experiment is random.
\begin{itemize}
  \item THe set of possible outcomes is the \textbf{sample space}  $\omega $ 
    \begin{itemize}
      \item An \textbf{event}  $A \subset \omega $  if the outcome is contained in $A$
      \item The \textbf{complement}  of an event $A$ is  $A^{c} = \omega  \setminus A$ 
      \item The \textbf{null event} $\emptyset$ is the empty set $\emptyset = \omega \setminus \omega $ 
    \end{itemize}
\end{itemize}

\subsubsection{Combining Event}%
\label{ssub:combining_event}

Let $A$ and B be events 
\begin{itemize}
  \item The \textbf{union} $A \cup  B$ is the event that at least one of $A$ and $B$ occur.
  \item the \textbf{intersection}  $A \cap B$ is the event that both $A$ and $B$ occur.
\end{itemize}

The events $A_{1}, A_{2}, \ldots$ are called disjoint (or \textbf{mutually exclusive} ) if $A_{i} \cap A_{j} = \emptyset$ for $i \neq j$

\subsubsection{Probability}%
\label{ssub:probability}

$Pr$ is called a probability on $\omega $ if 

\begin{itemize}
  \item Pr $\{ \omega \} = 1  $ 
  \item $0 \le P\left\{ A \right\} \le 1$ for all events $A$ 
  \item For $A_{1}, A_{2} , \ldots$ that are mutually exclusive \[
  P \left\{ \bigcup_{i = 1}^{\infty}A_{i}  \right\} = \sum_{i=1}^{\infty} P \left\{ A_{i} \right\}
  \] 
\end{itemize}
We call $P\left\{ A \right\}$ the probability of $A$.


\subsubsection{Law of total probability}%
\label{ssub:law_of_total_probability}

Let $A_{1}, A_{2}, \ldots$ be a partition of $\omega $ ie 
\begin{itemize}
  \item $\omega  = \bigcup_{i=1}^{\infty} A_{i}$
  \item $A_{1}, A_{2}, A_{3}, \ldots$ are mutually exclusive.
\end{itemize}

Then for any event $B$ \[
  P\left\{ B \right\} = \sum_{i=1}^{\infty} P\left\{ B \cap A_{i} \right\}
\] 

\textbf{This concept is very important.} 

\subsubsection{Independence}%
\label{ssub:independence_2s}
Event $A$ and $B$ are independent of \[
P\left\{ A\cap B \right\} = P\left\{ A \right\}P\left\{ B \right\}
\] 
Events $A_{1}, \ldots, A_{n}$ are independent if for any subset \[
P\left\{ \bigcap_{j=1}^{k} A_{i_j} \right\} = \prod_{j=1}^{k} P \left\{ A_{i_j} \right\} 
\] 

In this case $P\left\{ \bigcap_{i = 1}^{n} A_{1} \right\} =  \prod_{i = 1}^{n} P\left\{ A_{i} \right\} $


\newpage
\subsubsection{Random Variables}%
\label{ssub:random_variables}

\begin{definition}
  A \textbf{random variable}  is a real-vaued function on the sample space. Informally:  A random variable is a real valued variable that takes on its value by chance.
\end{definition}


\begin{tcolorbox}
  \textbf{Example.} 
  \begin{itemize}
    \item Throw two dice. $X = \text{sum of the two dice}$
    \item Throw a coin.  $X$ is $1$ for heads and $X$ is $0$ for tails.
  \end{itemize}
\end{tcolorbox}


\subsubsection{Notation for random variables}%
\label{ssub:notation_for_random_variables}

We use 
\begin{itemize}
  \item upper case letters such at $X$, $Y$ and $Z$  to represent random variables.
  \item lower case letters as $x$, $y$, $z$ to denote the real-valued realized value of a the random variable.
\end{itemize}

Expression such as $\left\{ X \le x \right\}$ denators the event that $X$ assumes a valye less than or earl to the real number x.

\subsubsection{Discrete random variables}%
\label{ssub:discrete_random_variables}

The random variable $X$ is \textbf{discrete}  if it has a finite or countablle number of possible outcomes $x_{1}, x_{2}, \ldots$ \par
\begin{itemize}
  \item The \textbf{probability mass function } $p_{x} \left( x \right) $ is given by \[
  p_{x}\left( x \right) = P \left\{ X = x \right\}
  \] and satisfies \[
  \sum_{i=1}^{\infty} p_{x}\left( x_{i} \right) = 1 \quad  \text{and} \quad  0\le p_{x} \left( x_{i} \right) \le  1 
  \] 
\item The \textbf{cumulative distribution function} (CDF) a of $X$ can be written \[
F_{x}\left( x \right) = P\left\{ X \le x \right\} = \sum_{i: x_{i} \le x}^{} p_{x}\left( x_{i} \right) 
\]  
\end{itemize}

\subsubsection{CFD}%
\label{ssub:cfd} 

The CDF of $X$ may also be called the \textbf{distrobution function}  of $X$ \par 
Let $F_{x}\left( x \right)$ be the CDF of $X$, then 
\begin{itemize}
  \item $F_{x}\left( x \right)$ is monetonaly increasing.
  \item $F_{x}$ is a stepfunction, which is a pieace-wise constant with jumps at $x_{i}.$
  \item $\lim_{x \to \infty} F_{x}\left( x \right) = 1$
  \item $\lim_{x \to - \infty} F_{x}\left( x \right) = 0$
\end{itemize}


\subsubsection{Continious random vairbales}%
\label{ssub:continious_random_vairbales}
 A \textbf{continious} random variables takes value o a continious scale.
 \begin{itemize}
   \item The CDF, $F_{x}\left( x \right) = P \left( X \le x \right)$ is continious.
   \item The \textbf{probability density function} (PDF) $f_{x}\left( x \right) = F_{x}' \left( x \right)$ can be used to calculate probablities \[
   \begin{split}
     Pr \left\{ a < X < b \right\} &=  Pr \left\{ a \le X < b \right\} = Pr\left\{ a < X \le b \right\} \\
     &=  Pr\left\{ a \le X \le b \right\} = \int_{a}^{b}  f_{x}\left( x \right)dx   
   \end{split} 
   \] 
 \end{itemize}


 \subsubsection{Important properties}%
 \label{ssub:important_properties}

 \begin{itemize}
   \item CDF:
     \begin{itemize}
       \item Monotonely increaing
       \item continious
        \item $\lim_{x \to \infty} F_{x} = 1$ and $\lim_{x \to - \infty} F_{x}\left( x \right) = 0$
     \end{itemize}
   \item PDF
     \begin{itemize}
       \item $f_{x}\left( x \right) \ge 0$ for $x \in\mathbb{R} $
       \item $\int_{-\infty}^{\infty} f_{x}\left( x \right)dx = 1$
     \end{itemize}
 \end{itemize}


\subsubsection{Expectation}%
\label{ssub:expectation}

Let $g: \mathbb{R}  \to \mathbb{R} $ be a function and $X$ be a random variable.
\begin{itemize}
  \item If $X$ is discrete, the expected value of $g\left( X \right) $ is \[
  E\left[ g\left( X \right) \right] =  \sum_{x: p_{x}\left( x \right)> 0}^{} g\left( x \right) p_{x}\left( x \right)  
  \] 
\item If $X$ is continous, the expected value of $g\left( X \right) $ is  \[
E\left[ g\left( X \right) \right] = \int_{-\infty}^{\infty} g\left( x \right)f_{x}\left( x \right) dx 
\] 
\end{itemize}

\subsubsection{Variance}%
\label{ssub:variance}

The variance of the random variable $X$ is \[
  Var\left[ X \right] =  E \left[( X - E\left[ X \right])^{2} \right] =  E\left[ X^2 \right] - E\left[ X \right]^2 
\] 
Important properties of expectation and variance.
\begin{itemize}
  \item Expectations is linear \[
  E\left[ aX + bY +c \right] = aE\left[ X \right] + bE\left[ Y \right] + c.
  \] 
\item Variance scales quadratically and is invaraient to the addition of constants \[
Var\left[ aX + b \right] = a^2 Var \left[ X \right] 
\] 
\item fir independent stochastic variables.\[
    Var \left[ X + Y \right] = Var \left[ X \right] + Var\left[ Y \right]
\] 
\end{itemize}

\subsubsection{Joint CDF}%
\label{ssub:joint_cdf}

If $\left( X,Y \right)$ is a pair for random variables, their \textbf{joint comulative distribution function } is given by \[
F_{X,Y} = F\left( x,y \right) =  Pr\left\{ X \le x \cap Y \le y \right\}
\]. 
\subsubsection{Joint distrubution for discrete random variables}%
\label{ssub:joint_distrobution_for_discrete_random_variables}
If $X$ and $Y$  are discrete, the \textbf{joint probability mass function } $ p_{x,y} = Pr\left\{ X = x, Y =y \right\} $. can be used to compute probabilities \[
Pr\left\{ a < X < b, c < Y \le d \right\} =  \sum_{a < x \le b}^{}  \sum_{c < y \le d}^{} p_{X,Y}   \left( x,y \right)
\] 

\subsubsection{Joint distrubution for continous random variables}%
\label{ssub:joint_distrobution_for_continous_random_variables}

If $X$ and $Y$ are continious the \textbf{joint probability density function}  \[
.f_{X,Y} \left( x,y \right) = f\left( x,y \right) = \frac{\partial ^2}{\partial x \partial y } F\left( x,y \right)   
\]  can be used to compute probabilities \[
Pr\left\{ a < X \le b,  \quad  c < Y \le d  \right\} = \int_{a}^{b} \int_{c}^{d} f\left( x,y \right)dxdy    
\] 

\subsubsection{Independence}%
\label{ssub:independence_3}

The random variables $X$ and Y are independent if \[
Pr\left\{ X \le a , Y \le b \right\} =  Pr\left\{ X \le a \right\} \cdot  Pr\left\{ Y \le b \right\}, \quad  \forall a,b \in  \mathbb{R}  
\] 
In terms of CDFs:  $F_{X,Y}(a,b ) =  F_{X}\left( a \right)\cdot F_{Y}\left( b \right) \quad  \forall a,b \in \mathbb{R}  $
\par
Thus we have 
\begin{itemize}
  \item $p_{X,Y} \left( x,y \right) = p_{X}\left( x \right) \cdot  p_{Y}\left( Y \right)$ for discrete random variables
  \item $f_{X,Y}\left( x,y \right) = f_{X}\left( x \right) \cdot  f_{Y}\left( Y \right)$ for continuous random variables.
\end{itemize}






 
 




\newpage
\section{Lecture 3}%
\label{sec:lecture_3}

\subsection{Randoms sum}%
\label{sub:randoms_sum}

Building on the hunter example from last week. we can more generally consider random sums \[
  X = \begin{cases}
    0,  &  \quad  N = 0 \\
    \zeta_{1} + \zeta _{2} + \ldots + \zeta_N , \quad  N >0  
  \end{cases}
\] 
where 
\begin{itemize}
  \item $N$ is a discrete random variable with values $0,1, \ldots$ 
  \item $\zeta _{1}, \zeta _{2}, \ldots $ are independent random variables
  \item $N$ is independent of $\zeta _{1}, \zeta _{2} + \ldots + \zeta _{N}$ 
  \item \textbf{Notation}  $X = \sum_{i=1}^{N} \zeta _{i} = \zeta _{1} + \zeta _{2} + \ldots + \zeta _{N}$ 
\end{itemize}

\begin{tcolorbox}
  \textbf{Example.} 
  \begin{enumerate}
    \item Insurance company \[
    N: \text{ Number of claims.} 
    \] 
  \[
    \zeta _{1} , \zeta _{2} , \ldots \quad  : \quad \text{Sizes of the claims} 
  \] 

  Total liabilility: \[
  X = \zeta _{1}+ \zeta _{2} + \ldots + \zeta _{N}
  \] 
\item  Be careful! \[
    \begin{split}
      \overbrace{E\left[ \sum_{i=1}^{N} \zeta _{i} \right]}^{\neq \sum_{i=1}^{N} E\left[ \zeta _{i} \right]}   & = E\left[ E\left[ \sum_{i=1}^{N} \zeta _{i}  \mid N \right] \right]\\
&= E\left[ \sum_{i=1}^{N} E\left[ \zeta _{i}  \mid  N \right] \right] 
    \end{split} 
\] 
  \end{enumerate}
\end{tcolorbox}

\subsection{Self Study}%
\label{sub:self_study}

Section 2.2, 2.3, 2.4

\subsection{Stochastic process in descrete time}%
\label{sub:stochastic_process_in_descrete_time}
\begin{definition}
  A \textbf{discrete-time stochastic process}  is a family of random variables $\left[ X_{t} : t \in  T \right]$ where $T$ is discrete.
  \begin{itemize}
    \item We use $T = \left\{ 0,1,2,.. \right\}$ and write $X_{n}$ instead of $X_{t}$
    \item  we call $X_{n}$ the \textbf{state}  at time $n =  0,1,2,3, \ldots$
    \item We call the set of all possible states the \textbf{state space} 
  \end{itemize}
\end{definition}

\begin{table}[htpb]
  \centering
  \caption{Table for example}
  \label{tab:label}
  \begin{tabular}{l|cccc}
    Day & $n =0$ & $n=1$ & $n=2$ & \ldots \\ 
    Random Variable  & $X_{0} $ & $X_{1}$ & $X_{2}$ & \ldots \\
    Realization  1& $x_{0} = 0$ & $x_{1} =1$ &  $x_{2} = 1 $ & \ldots \\
    Realization 2 & $x_{0} = 1$ & $x_{1} =1$ &  $x_{2} = 1 $ & \ldots \\
  \end{tabular}
\end{table}
\begin{tcolorbox}
  \textbf{Example.}  \[
  X_{n} = \begin{cases}
    1 ,  &  \quad \text{if it rains on day } n \\
    0,   &  \quad     \text{no rain on day } n
  \end{cases}
  \] 
  State space $= \left\{ 0,1 \right\}$
  \par
  \textbf{We have a problem.} Need \[
  Pr \left \{ X_{n} = x_{n}  \mid  X_{n-1} = x_{n} , X_{n-2} = x_{n-2}, \ldots, X_{0} = x_{0} \right \}.
  \]    for all $n = 0,1,2,\ldots$

\end{tcolorbox}

\subsection{Markov chain}%
\label{sub:markov_chain}


\begin{definition}[Discrete time Markov Chain]
  A \textbf{ Discrete time markoc chain}  is a discrete time stochastic process $\left\{ X_{n} : n = 0,1,\ldots \right\}$ that statisfied the \textbf{markov property}  such that \[
  \begin{split}
       & Pr \left \{ X_{n-1} = j  \mid  X_{n} = i ,    X_{n-1} = i_{n-1} , \ldots, X_{0} = i_{0} \right \}  \\
    &=  Pr \left \{ X_{n+1} = j  \mid  X_{n} = i \right \}  
  \end{split} 
  \] 
  for $n = 0,1,2,3, \ldots$ and for all states $i$ and $j$
\end{definition}

\begin{definition}[One-step transition probabilities]
  We can define it  as 
  \begin{itemize}
    \item For a discrete Markov chain $\left\{ X_{n}: n= 0,1,2, \ldots \right\}$ we call $P_{ij}^{n, n+1} = Pr \left \{ X_{n+1} = j , X_{n} =i \right \} $ the \textbf{one step trainsition probabilities} . 
    \item We will assume \textbf{stationary transition probabilities} , i.e that \[
    P_{ij}^{n, n+1} = P_{ij}
    \]   for $n = 0,1,2, \ldots$ and all states $i $ and $j$ . 
  \end{itemize}
\end{definition}

Some of the properties 
\begin{enumerate}
  \item "You will always go somewhere" \[
  \sum_{j}^{}  P_{ij} = 1 \quad  \forall i 
  \] 
\item The markov chain can be described as follows. \[
    \begin{split}
  & Pr \left \{ X_{0} = i_{0} , X_{1} = i_{1}, \ldots, X_{n} = i_{n} \right \}   \\
 &=  Pr \left \{ X_{0} = i_{0}  \right \}   Pr \left \{ X_{1} = i_{1}  \mid  X_{0} = i_{0} \right \}   \ldots \\
     & \quad \quad    Pr \left \{ X_{n} = x_{n}  \mid  X_{n-1} = i_{n-1} \ldots X_{0} = i_{0} \right \}  \\
  &  \quad \vdots \quad     \text{Markov step} \\
 &=  Pr \left \{ X_{0} = i_{0}  \right \}  \cdot  Pr \left \{ X_{1} = i_{1}  \mid X_{0} = i_{0} \right \} \ldots \\
  & \quad \quad    Pr \left \{ X_{n} = x_{n}  \mid  X_{n-1} = i_{n-1} \right \}   \\
 &=  Pr \left \{ X_{0} = i_{0}  \right \} P_{i_{0}, i_{1}} \cdot  P_{i_{1}, i_{2}} \ldots P_{i_{n-1}, i_{n}}
    \end{split} 
\] 
Which is a major simplification.
\end{enumerate}

\begin{definition}[Transition Probability Matrix] \quad
  For a discrete time markov-chain with state space $\left \{ 0,1, \ldots, N \right \}$ we call
  \[
  \mathbf{P} = \begin{bmatrix} 
    P_{00} & \ldots & P_{0N} \\
    P_{10}  & \ldots \\
    \vdots  &   &  \ddots \\
    P_{N0} & \ldots & P_{NN} 
  \end{bmatrix} 
  \] 
  Is the transition matrix.
  For statespace $\left\{ 0,1,2, \ldots \right\}$ we envision an infinitely sized matrix.
\end{definition}

 \begin{tcolorbox}
   \textbf{Example.} 
   \begin{itemize}
     \item Markoc chain : $\left\{ X_{n} : n = 0,1,2,\ldots \right\}$
     \item State space  $= \left\{ 0,1 \right\}$
     \item Transition Matrix \[
     \mathbf{P} = \begin{bmatrix} 
     0.9  &  0.1 \\
     0.6  &  0.4
     \end{bmatrix} 
     \] 
   \end{itemize}
   We can compute \[
     \begin{split}
        Pr \left \{ X_{3} = 1  \mid  X_{2} = 0 \right \} &=  p_{01} \\
        &= 0.1   \\
        Pr \left \{ X_{10} = 0  \mid  X_{9} = 1 \right \} &=  P_{10} \\
        &= 0.6  \\
     \end{split} 
   \] 
 \end{tcolorbox}

\begin{definition}[Transition Diagram]
  Let $\left\{ X_{n}: n = 0,1, \ldots \right\}$ be a discrete time Markov chain.  A \textbf{state trasnistion diagram} visualizes the transition probabilities as a weighted directed graph where the nodes are the states and the edges are the possible transitions marked with the transistion probabilities.
\end{definition}

\begin{tcolorbox}
  \textbf{Example.} State space $= \left\{ 0,1,2 \right\}$ and \[
  P = \begin{bmatrix} 
  0.95  & 0.05 & 9 \\
  0  & 0.9  &  0.1 \\
  0.01  &  0  &  0.99
  \end{bmatrix} 
  \] 
  Transisition diagram 
  \begin{tcolorbox}
    Nice figure of the diagram
  \end{tcolorbox}
\end{tcolorbox}

 \subsection{Doing n transitions.}%
 \label{sub:doing_n_transitions_}

 \begin{theorem}
   For a Markoc chain $\left\{ X_{n}: n= 0,1, \ldots \right\}$ and any $m\ge 0$ we have \[
     Pr \left \{ X_{m-n} = j  \mid X_{m} = i  \right \}  = P _{ij}^{(n)} =  \sum_{k=0}^{\infty}  P _{ik} P_{kj}^{(n-1)} ,  \quad  n>0 
   \] 
   where we define \[
   P_{ij}^{(0)} = \begin{cases}
     1 , \quad  i= j \\
     0, i \neq j 
   \end{cases}
   \] 
 \end{theorem}

 \begin{proof}
   Set $m = 0$ then is \[
   \begin{split}
     P_{ij }^{(n+1)}   & = Pr \left \{ X_{n+1} = j  \mid  X_{0} = i \right \}   \\
     &= \sum_{k}^{}  Pr \left \{ X_{n+1} = j, X_{1} = k  \mid  X_{0} = i \right \}   \\
     &=  \sum_{k}^{} Pr \left \{ X_{n+1} = j  \mid  X_{1} = k, X_{0} = i \right \} \cdot Pr \left \{ X_{1} = k  \mid  X_{0} = i \right \}   \\
     &= \sum_{k}^{} P_{kj}^{(h)} \cdot P_{ik}  = \sum_{k}^{}  P_{ik} P_{kj}^{(h)}
   \end{split} 
   \] 
 \end{proof}
 
 \begin{tcolorbox}
   \textbf{Example.} $\left\{ X_{n} : n= 0,1,2, \ldots \right\}$ is a markoc chain and \[
   P = \begin{bmatrix} 
   0.1  &  0.9 \\
   0.6  &  0.4 
   \end{bmatrix} 
   \] 
   Find $P_{01}^{(4)}$ .
   \textbf{Solution}. \[
   P^2 = \begin{bmatrix} 
   0.55  &  0.45 \\
   0.30  &  0.70
   \end{bmatrix} 
   \] 
   So by doing matrix multiplication and we end up with \[
   P^{4} = P^{2} \cdot  P^{2} = \begin{bmatrix} 
   0.4375  &  0.5625 \\
   0.3750  &  0.6250
   \end{bmatrix} 
   \] 
   Which therefore ends up with the answer \[
   P_{01}^{(4)} = 0.5625
   \] 
 \end{tcolorbox}
 




\newpage

\section{Lecture 4}%
\label{sec:lecture_4}

 \subsection{Introduction to first step analysis}%
 \label{sub:introduction_to_first_step_analysis}

 \textbf{Input} 
 \begin{itemize}
   \item $i_{0}$ : starting state
    \item $P$ : transition probability matrix
    \item T: number of time steps
 \end{itemize}
 \textbf{Algorithm} 
 \begin{enumerate}
   \item Set $x_{0} = i_{0}$
   \item for $n=1 \ldots T$
   \item $\quad   $ Simulate $x_{n}$ from $X_{n}  \mid  X_{n-1} = x_{n-1}$
   \item end
 \end{enumerate}
 
 \textbf{output} : One realization $x_{0}, x_{1} , \ldots, x_{T}$ 
 
 \begin{tcolorbox}
   \textbf{Example.} 
   \[
   P = \begin{pmatrix}
   0.95  &  0.05  &  0 \\
   0  &  0.90  &  0.10 \\
   0.01  &  0  &  0.99
   \end{pmatrix} 
   \] 
   Let $x_{0} = 0$
   \begin{enumerate}
     \item $x_{0} = 0$  
     \item 
       \begin{align*}
       Pr \left \{ X_{1} = 0 | X_{0} = 0 \right \} = &  P_{00} = 0.95  \\
       Pr \left \{ X_{1}  \mid  X_{0}  \right \}  &=  P_{01} = 0.05 \\
       Pr \left \{ X_{1}  \mid  X_{0} = 0 \right \}  &=  P_{02} = 0 \\
       .\end{align*}
       Assume we get $x_{1} = 1$
     \item States 
       \begin{itemize}
         \item \[
             \begin{split}
         0: P_{10}  &=  0 \\
         1: P_{11 } &=  0.90 \\
         2: P_{12} &=  0.10 \\
         \vdots  \\
             \end{split} 
         \] 
       \end{itemize}
   \end{enumerate}
 \end{tcolorbox}

 \begin{tcolorbox}
   General notes on simulation 
   \begin{itemize}
     \item

   $Pr \left \{ A  \right \} \approx \frac{\text{# times A occure}}{ \text{# Simulations}}  $
 \item $E\left[ X \right] \approx \frac{1}{N}  \sum_{i=1}^{B}  x_{i}$
   \end{itemize}
 \end{tcolorbox}

   \textbf{Example.} We have $N=100$ divided into two containers labelled $A$ and $ b$. At each time $n$, one ball is selected at random and moved to the container. Let $Y_{n}$ denote the number of balls in container $A$ at time $n$, and define $X_{n} = Y_{n} -50$. Find the transition probabilities and simulate and plot one realization of \[
   \left\{ X_{n}: n  = 0,1, \ldots, 500 \right\}
   \] 

   \textbf{Answer} 
 
\begin{figure}[ht]
    \centering
    \incfig{balls}
    \caption{balls}
    \label{fig:balls}
\end{figure}

\begin{itemize}
  \item Only move One ball
  \item Can move only from $i$ to $ j = i-1$ or  $j i +1$
\end{itemize}
\[
P_{ij} = \begin{cases}
  \frac{50 -i}{ 100 }   & , \quad  j = i+1 \\
   \frac{50+i}{100}   & , j = i-1 \\
   0  & , \text{otherwise}.
\end{cases}
\] 

% Visualization
% \begin{figure}[ht]
%     \centering
%     \incfig{vizzz}
%     \caption{vizzz}
%     \label{fig:vizzz}
% \end{figure}

\textbf{Motivation}  
\begin{definition}
  For a markov chain, a state $i$ sich that $P_{ij} = 0 \forall j\neq i$  is called \textbf{absorbing.} 
\end{definition}
 \begin{tcolorbox}
   \textbf{Example.} Let $\left\{ X_{n} \right\}$ be a Markov chain woth transition probability matrix \[
   \mathbf{P} = \begin{pmatrix}
   1  &  0  &  0 \\
   \alpha   &  \beta  &  \gamma  \\
   0  & 0  & 1
   \end{pmatrix} 
   \] 
   where $\alpha , \beta , \gamma > 0$ and $\beta = 1- \alpha -\gamma $. Assume $x_{0} = 1$
   \begin{enumerate}
     \item  What is the expected time until absortion ?
     \item What is the probability to be absorbed in state $0$ ?
   \end{enumerate}

   \textbf{Realization} . \[
   \overbrace{1,1,1,1,1,2}^{4 \text{ steps to absorption}} ,2,2 \ldots
   \] 
   \textbf{Mathematically} 
   
   \newpara
   Let $T = \min_{} \left\{ n \ge 0 : X_{n} = 0 \quad \text{or} \quad  X_{n} = 2   \right\}$. Then is \[
   \begin{split}
     Q1:  &  \quad E\left[ T  \mid X_{0} = 1 \right]  \\
     Q2:  &  \quad  Pr \left \{ X_{T} = 0  \mid  X_{0} = 1 \right \}  
   \end{split} 
   \] 
   The idea of first step analysis is to define 
   \begin{itemize}
     \item $T^{(n) } = \min_{}  \left\{ n \ge :: X_{m\times n } = 0 \quad \text{or} \quad X _{m+b} =2   \right\}$
     \item $T = T^{(0)}$
     \item $v^{(m)}_{i} = E\left[ T^{(m) }  \mid  X_{m}  = i \right]$
     \item $v_{i} = v^{(0)} _{i}$
   \end{itemize}


 \end{tcolorbox}
   \begin{table}[htpb]
     \centering
     \caption{Let $m$ be timesteps}
     \label{tab:label}
     \begin{tabular}{l|ccccc}
     $m$   & $0$  & $2$   &3  &4  & 5  \\
     $v^{(m)}_{0}$   &  $0$ & $0$   & $0$  & $0$  & $0$  \\
     $v^{(m)}_{1}$   & $v_{1}$  &  $v_{1}$& $v_{1}$ & $v_{1}$ &$v_{1}$  \\
     $v^{(m)}_{2}$   & $0$ & $0$  &  $0$& $0$  & $0$  \\
     \end{tabular}
   \end{table}

   \textbf{First step analysis for Q1} 
   \[
     \begin{split}
   v_{i}  & = \sum_{k=0}^{2}  Pr \left \{ X_{1} = k  \mid  X_{0} = i \right \}  \left( 1 + v_{k} \right) \\
&= \sum_{k=0}^{2}  P_{ik} \left( 1+ v_{k} \right) = \sum_{k=0}^{2} P_{ik} v_{k} +1 \quad  \text{which is true for } \quad i = 0,1,2   \\
     \end{split} 
   \] 
   Which is reduced to linear algebra. Solving it by \[
   \begin{split}
     v_{0} &=  v_{2} = 0 \\
     \implies  v_{1} &= \alpha  v_{0} + \beta v_{1} + \gamma v_{2} + 1 \\
     \implies  v_{1} &=  \frac{1}{ 1- \beta } \quad  \text{[Q1]}  \\
   \end{split} 
   \] 
   \begin{tcolorbox}
     $P_{ij} \implies  i = \text{row} , \quad j = \text{column} $
   \end{tcolorbox}
   First step analyis and let \[
     \begin{split}
   u_{i}  & = Pr \left \{ X_{T} = 0  \mid  X_{0} = i \right \}  \\
     &  \downarrow \\
   u_{i} &= \sum_{k=0}^{2}  P_{ik} u_{k}, \quad i = 0,1,2   \\
     \end{split} 
   \] 
     \begin{itemize}
       \item Easy: $u_{0} = 1, u_{2} =0$
       \item Harder: $u_{1} = \alpha  u_{0} + \beta u_{1} + \gamma u_{2}$ such that \[
           u_{1} = \alpha \frac{1}{1- \beta }  = \frac{\alpha }{\alpha  - \beta }  \quad \text{[Q2]} 
       \] 
     \end{itemize}
   
     \begin{tcolorbox}
       \textbf{Example.} let $\left[ X_{n} \right]$ be a markov chain with transition matrix \[
       \mathbf{P} = \begin{pmatrix}
       1  &  0 &  0  &  0 \\
       0.4  &  0.3  &  0.2  &  0.1 \\
       0.1  &  0.3  &  0.3 &  0.3 \\
       0  &  0 &  0 &  0 
       \end{pmatrix} 
       \] 
       The starting state is $x_{0} = 1$. Calculate the probability to be absorbed in the state $D$.
       \begin{enumerate}
         \item Define $u_{i} = Pr \left \{ \text{absorbed in state 0}  \mid  X _{0} = i \right \} $ for $i = 0,1,2,3$ 
         \item Get the easy ones out of the way. In this case $u_{0} = 1$ and $u_{3} = 0$
         \item 
           \[
           \begin{split}
            u_{1}  &  = P_{10} u_{0} + P_{11} u_{1} + P_{12} u_{2} + P_{13}u_{3}  \\
            &= 0.4 + 0.3 u_{1} + 0.2 u_{2}  \\
            u_{2} &= P_{20} u_{0} + P_{21} u_{1} + P_{22} u_{2} + P_{23} u_{3} \\
            &= 0.1 + 0.3 u_{1} + 0.3 u _{2} \\
           \end{split} 
           \] 
         \item Solve for $u_{1}$ and $u_{2}$
       \end{enumerate}
     \end{tcolorbox}
 
\newpage
\section{Lecture 5}%
\label{sec:lecture_5}

\textbf{Example.} Let $P$ be the matrix \[
P = \begin{bmatrix} 
1  & 0  &  0 &  0 \\
0.4  &  0.3  &  0.2  &  0.1 \\
0.1   &  0.3  &  0.3  &  0.3 \\
0  &  0  &  0 &  0
\end{bmatrix} 
\] 
With starting state $x_{0} = 1$

\begin{enumerate}
  \item Define $T  = \min_{ n\ge 0 : X_{n} = 0 \quad X_{n} = 3 } $ and $v_{i} = E \left[ T  \mid  X_{0} = i \right] $ for $i  = 0,1,2,3 $
  \item Set $v_{0} = v_{3} = 0$
  \item \[
  v_{1} = P_{10} v_{0} + P_{11} v_{1} + P_{12} v_{2} + P_{13} v_{3}  = 0.3 v_{1} + 0.2_{v2} +1
  \] 
  and \[
  v_{2} = P_{20 } v_{0}  + P_{21} v_{1} + P_{22}v_{2} + P_{23 } v_{3} + 1 = 0.3 v_{1} + 0.3 v_{2} + 1
  \] 
\item Solve the equations and end up with \[
v_{1} = \frac{90}{43} \quad \text{and} \quad v_{2} \frac{100}{43}  
\] 

\end{enumerate}


\begin{theorem}
  Let $\left\{ X_{n} \right\}$ be a discrete time Markov chain with state space $S = \left\{ 0, 1, \ldots , N \right\} $ and transition probability matrx $\mathbf{P}$. Let $A \subset S$ be the set of absorbing state. Then
  \begin{enumerate}
    \item If $v_{i}$ is the expected time to absorption conditional on $X_{0} = i$ then \[
    \begin{split}
      v_{i}  & = 0, \quad  i \in  A   \\
      v_{i} &=  1+ \sum_{ i \in  \mathbb{R} }^{} P_{ik} v_{k} \quad i \in  A^{c}  \\
    \end{split} 
    \] 
  \end{enumerate}
\end{theorem}


\textbf{Example.} 
A gambler has $10 \$$ and bets  $1\$$  If he wins the round, his fortune increases $ 1 \$$.  The probability of winning each round is  $ 0  < p < 1$ and the probability of losing each round is $q = 1 - p$. The gambler will continue gambling until his fortine is \$ $N$ or $0 \$$ where  $N > 10$. What is the probability the gambler will be ruined. 

\begin{enumerate}
  \item Extract the essential stuff. \[
      \begin{split}
        X_{n} &=  \text{Fortune at time} \quad  n, \quad n = 0,1,2,\ldots   \\
        \text{State space } &=  \left\{ 0,1, \ldots, N \right\} \\
        \text{Target: } u_{k}   & = Pr \left \{ \text{Absorption in state 0}  \mid X_{0} = k \right \}, \quad  k = 0,1, \ldots, N \\
      \end{split} 
  \] 
\item  Visualize the transitions.  Insert figure of tranistions.
   \item Make the eprobability matrix. The rows are "to" and the columns are "1"
     \[
     P = \begin{bmatrix} 
     1   &  0 & 0 & 0  & \ldots  &  0 \\
     q  & 0  &   p  & 0  &   \ldots  &  0 \\
     0  &  q  &   0  &  p  &  \ldots   \\
     
     \vdots   &  &   \ddots    \\
      &  &  & q  & 0  &  p \\
     0  &  0  &  \ldots  &   &  & 1
     \end{bmatrix} 
     \] 
   \item Set up the iteration \[
   \begin{split}
     u _{0}  & = 1 , \quad  u_{N} = 0 , \quad  \text{Easy}   \\
     u_{i} &=  P_{i, i.1} u_{i-1} + P _{i,i+1} u_{i+1}  \\
     &= q u_{i-1} + p u_{i+1} , \quad i=1,2, \ldots, N-1  \\
   \end{split} 
   \] 
 \item 
   \begin{enumerate}
     \item  \[
         \begin{split}
           \overbrace{(p + q)}^{ = 1} u_{i}   & = q u_{i -1} + p u_{i+1} \\
    q\left[ u _{ i} - u_{i-1} \right] &=  p \left[ u_{i+1} - u_{i} \right] \\
      &  \downarrow \quad \text{Trick}    \quad \chi _{i} = u_{i} - u_{i-1}   \\
    q \chi _{1} &=  p \chi _{i+1} , \quad  \implies  \chi _{i+1} = \frac{q}{p}  \chi _{i}  \quad  i = 1,2 , \ldots , N  \\
         \end{split} 
     \] 
   \item \[
   \begin{split}
     \chi _{1} + \chi _{2} + \ldots + \chi _{k} &= \left[ u - u_{0} \right] + \left[ u_{2} - u_{1} \right] + \ldots + \left[ u_{k} - u_{k-1} \right] \\
       &  \downarrow  \quad \text{Telescoping sum}  \\
        \chi _{1}\left[ 1 + \frac{q}{p} + \left( \frac{q}{p} \right)^2 + \ldots + \left( \frac{q}{p} \right)^{k-1}  \right]&=  u_{k} -1, \quad k = 1,2,3,\ldots,N  \\
   \end{split} 
   \] 
   For $k = N$ : \[
   \begin{split}
     \chi _{1} &=    \frac{u_{N} -1}{\sum_{k=0}^{N-1} \left( \frac{q}{p} \right)^{k}}  = \frac{-1}{ \sum_{k = 0}^{N-1}  \left( \frac{q}{p} \right)^{k}}   \\
     &= \begin{cases}
       -\frac{1}{N}\quad   & , q = p = \frac{1}{2} \\
       \frac{- \left( 1- \frac{q}{p} \right)}{\left( 1- \left( \frac{q}{p} \right) \right)} \quad   &  q\neq p \\
     \end{cases} \\
   \end{split} 
   \] 
 \item 
   From  the telescoping sum 
   \begin{align*}
     u_{k} &=  1+ \chi _{1} \sum_{i=0}^{k-1} \left( \frac{q}{p} \right)^{i} \\
     &=  \begin{cases}
       1-\frac{1}{N} \cdot k =  \frac{N-k}{N}  ,  &  \quad  p = q = \frac{1}{2}  \\
       1- \frac{1- \left( \frac{q}{p}  \right)^{k}}{ 1- \left( \frac{q}{p} \right)^{N}} = \frac{\left( \frac{q}{p} \right)^{k} - \left( \frac{q}{p} \right) ^{ N}}{1- \left( \frac{q}{p} \right) ^{N}}  ,   & \quad  p\neq q 
     \end{cases}, \quad \text{where} \quad    k = 1,2, \ldots.  \\
   .\end{align*}
   \end{enumerate}
 \item The final step \[
 u_{10} = \begin{cases}
   \frac{N- 10}{ N}  ,  &  \quad  p = q = \frac{1}{2} \\
   \frac{\left( \frac{q}{p} \right)^{10} - \left( \frac{q}{p} \right) ^{N}}{ 1 - \left( \frac{q}{p} \right)^{N}}  , &\quad     q \neq p 
 \end{cases}
 \] 
\end{enumerate}

\begin{remark}
  \begin{itemize}
    \item When $N \to  \infty$ \[
        \begin{split}
    q \ge p \quad   & \implies  \quad  \text{Almost certain you will loose.}   \\
    q < p  & \implies  P\left( \text{ruined} \right) = \left( \frac{q}{p} \right)^{10} \\
        \end{split} 
    
    \] 
  \end{itemize}
\end{remark}

\subsection{Markov Chain in infinitive time}%
\label{sub:markov_chain_in_infinitive_time}

\begin{definition}
  \textbf{Regular Markov Chain} . Consider a Markov chain $\left\{ X_{n}: \quad  n = 0,1,\ldots  \right\}$ with finite state space $ \left\{ 0,1,2, \ldots \right\}$ and transition matrix $\mathbf{P}$. IF there exists an integer $k >0$ so that all regular elements $\mathbf{P}^{k}$ are strictly positive, we call $\mathbf{P}$ and $\left\{ X_{n} \right\}$ regular. 
\end{definition}

\begin{remark}
   \begin{enumerate}
     \item $P$ is regular means that it exists an $k > 0$ so that $P^{(k)} _{ ij} > 0 \quad  \forall i,j $
     \item  If $P^{(k)} _{ij} \quad  \forall i,j $, then is $P^{(k)} _{ij} > 0 \quad  \forall i,j $ and $K \ge k$ 
   \end{enumerate}
\end{remark}


\section{References}
\label{sec:references}



\bibliographystyle{plain}
\bibliography{references}
\end{document}

