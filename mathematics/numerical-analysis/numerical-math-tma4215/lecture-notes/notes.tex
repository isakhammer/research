\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Numerical Maths}
\author{isakhammer }
\date{A20}

%%%% DEPENDENCIES v1.3 %%%%%%

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{mathtools}
%\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{todonotes}
\usepackage{esint}
\usepackage{float}


\usepackage{hyperref} 
\hypersetup{
  colorlinks=true, %set true if you want colored links
  linktoc=all,     %set to all if you want both sections and subsections linked
  linkcolor=blue,  %choose some color if you want links to stand out
} 
\hypersetup{linktocpage}


% inscape-figures
\usepackage{import}
\usepackage{pdfpages}
\usepackage{transparent}
\usepackage{xcolor}
\newcommand{\incfig}[2][1]{%
\def\svgwidth{#1\columnwidth}
\import{./figures/}{#2.pdf_tex} } \pdfsuppresswarningpagegroup=1

% Box environment
\usepackage{tcolorbox}
\usepackage{mdframed}
\newmdtheoremenv{definition}{Definition}[section]
\newmdtheoremenv{theorem}{Theorem}[section]
\newmdtheoremenv{lemma}{Lemma}[section]

% \DeclareMathOperator{\span}{span}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
%\newtheorem{example}{Example}

\newcommand{\newpara}
  {
  \vskip 0.4cm
  }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{document}
\maketitle
\tableofcontents
\newpage

\newpage
\section{Lecture 1}%
\label{sec:lecture_1}

\subsection{Practical Information}%
\label{sub:practical_information}

\begin{itemize}
  \item Brynjulf Owren, room 1350, Sentralbygg 2, brynjulf.owren@ntnu.no
  \item Alvar Lindell, room 1201, Sentralbygg 2, alvar.lindell@ntnu.no
\end{itemize}

There will be a total of 6 assignment where 4 should be approved. It should be delivered in blackboard as a jupyter notebook file including some control questions.   


\begin{itemize}
  \item \textbf{Project 1} It counts 10 procent on the final grade, relativaly small work, but somewhat large assignment. Every student submits her own separate .ipynb file. Discuss problem if you like, but make your own write-up. Likely to be a topic of algebra. Deadline. 10-15 September.
  \item \textbf{Project 2} Counts 20 procent on the final grade. Group project 1-3 students. Numerical ODE and may some optimization. 
\end{itemize}


\par
Lecture contents of the course
\begin{itemize}
  \item Introduction 3.6\%
  \item Numerical linear algebra 21.4\%
  \item Numerical ODE 28.6\%
  \item Nonlinear Systems and Numerical Optization 7.1\%
\end{itemize}

\textbf{May be jupyter programming on the exam.}  

\subsection{M2 Basic Linear Algebra}%
\label{sub:m2_basic_linear_algebra}

\subsubsection{Background summary}%
\label{ssub:background_summary}

\textbf{Vectors}. Most of the time we think of vectors as $n$-plets of real numbers.\[
v = \begin{bmatrix} 
v_1 \\
v_2 \\
\vdots \\
v_n
\end{bmatrix} 
\]   

Vecotrs are columns vectors if row vectors are needed use. \[
v^{T} = \begin{bmatrix} 
  v_1 & v_2 & v_3 & \ldots & v_{n}
\end{bmatrix} 
\]    

Linear Transformations are given by $A: \mathbb{R}^{n} \to \mathbb{R}^{m}$. These are represented ass $m \times n $ matrices.  $A = \left( \left( a_{ij} \right) \right)$ such that $ 1 \le i \le m$ and $1 \le j \le n$ . Notation $A \in \mathbb{R}^{m \times n }$ \[
  (Av)_{i} = \sum_{j=1}^{n} a_{ij} v_{j}, \quad  i = 1,\ldots, m. 
\] 

If $A = \left( \left( a_{ij} \right) \right)$, B $\left( \left( b_{ij} \right) \right)$ then $A +B = C$, $C = \left(\left( c_{ij} \right)  \right)$, $c_{ij} = a_{ij} + b_{ij}$.  

\par

Given to matrices, $A \in \mathbb{R}^{m \times k }$ and $B \in \mathbb{R}^{k\times n }$ \[
\mathbb{R}^{n} \to \mathbb{R}^{k} \to\mathbb{R}^{m}
\] 
\[
\mathbb{R}^{n} \to \mathbb{R}^{m}
\] 
\[
\left( A\cdot B \right)_{ij} =  \sum_{r=1}^{k}a_{ir}b_{ri}
\] 

\todo{ Fix a way to have notation on top of arrow and a better snippet for the summation. Might also train making quick vector notations. }

\subsubsection{Linear Independence}%
\label{ssub:linear_independence}

Let assume that we have $v_{1}, \ldots, v_{k}$ be vectors in $\mathbb{R}^{n}$ and let $\alpha_1, \alpha_2, \ldots , \alpha_{k}$ be scalar if \[
\sum_{n=1}^{k} \alpha_i v_i = 0 \quad \text{then is} \quad \alpha_1=\alpha_2 = \ldots = 0   
\] 
Then $v_1, v_2, \ldots, v_{k}$ is linear independent. 

\subsubsection{Inverse of an matrix}%
\label{ssub:inverse_of_an_ntimes_n_matrix}

If there is a matrix $B \in \mathbb{R}^{n\times n }$ such that \[
A\cdot B= B\cdot A = I
\] Then $B$ is the inverse of A.
B is denoted $B = A ^{-1}$ 
Basis of $\mathbb{R}^{n}$. Any set of $n$ linearly independent vectors in $\mathbb{R}^{n}$ is called a basis. \par

\subsubsection{Permutation Matrix}%
\label{ssub:permutation_matrix}


\textbf{Permuation Matrix.} Let $I \in \mathbb{R}^{n\times n }$ be the identity matrix. $I$ has columns $e_1, e_2, \ldots , e_n$ where $e_i$ is the $i$-th canonical unit vector \[
\begin{bmatrix} 
  0 & 0 & \ldots 1 \ldots 0 
\end{bmatrix} 
= e^{T}
\] 


Let $p = \begin{bmatrix} 
i_1, i_2, \ldots, i_n
\end{bmatrix}^{T} $ 
Be a permutation of the set $\left\{ 1, \ldots, n \right\}$ then \[
P = \begin{bmatrix} 
  e_1 & e_2 & e_2 
\end{bmatrix} 
\] 
The permutation matrix. 

\todo[inline]{ Implement example snippet }

The inverse of a permutation matrix in $P^{-1} = P^{T}$ and $\left( P^{-1} \right)_{ij} = P_{ji}$.

\subsubsection{Types of Matrices}%
\label{ssub:types_of_matrices}

\begin{itemize}
  \item Symmetric: $A^{T} = A$
  \item Skew symmetric: $A^{T} = -A$
  \item Orthogonal. $A^{T} A = I$
\end{itemize}


\newpage
\section{Lecture 3 -  August 25 - 2020}%
\label{sec:lecture_3}

\subsection{Continuation of previous lecture}%
\label{sub:contiouation_of_previous_lecture}


 Lets find a practical computation of $p^{(0)}, p^{(1)}, \ldots$. Always start with $p^{(0)} = r^{(0)} = b -Ax^{(0)}$. Suppose that $p^{(0)}, \ldots , p^{(k)}$ have been found. Set $p^{(k+1)} = r^{(k+1)} - b_{k} p ^{(k)}$. Require that \[
   \begin{split}
 0 &=   \left<p^{(k)} , p^{(k+1)} \right> _{A} = \left< p^{(k)}, r^{(k+1)}   \right> - \beta _{k} \left<p^{(k)} , p^{(k)} \right> \\
   &  \text{so } \quad  \beta _{k} =  \frac{\left<p^{(k)} , r^{(k+1)} \right> _{A}}{ \left<p^{(k)} , p^{(k)} \right>_{A}}     
   \end{split} 
 \] 

 Note that $x^{(k+1)} = x^{(k)} + \alpha _{k} p^{(k)} $  and \[
   \begin{split}
 b -Ax^{(k+1)} &=   b - Ax^{(k)} - \alpha _{k} Ap^{(k)} \\
  &  \underbrace{r^{(k+1)} =   r^{(k)} - \alpha  _{k} Ap^{(k)}}_\text{essential} 
   \end{split} 
 \] 
 Let $V_{k} = span \left\{ p^{(0)} , \ldots, p^{(k)} \right\}$ and since $r^{(0)} = p^{(s)}, \quad r^{(k+1)} = p ^{(k+1)} - \alpha _{k} A p^{(k)}   $ , it happens that $Ap^{(k)} \in  V_{k+1}$, we have \[
   V_{k} = span \left\{ r^{(0)} , \ldots, r^{(k)} \right\}
 \] 
 We want to prove that $\left<p^{(k+1)} , p^{(j)} \right> = 0 $ for $j = 0, \ldots, k-1$ \[
   \left<r^{(k+1)}- \beta _{k} p^{(k)} , p^{(j)} \right> _{A} = \left<r^{(k+1)}, p^{(j)} \right> - \beta _{k}\left< p^{(k)} , p^{(j)} \right>_{A}
 \] 
 We know that \[
   \begin{split}
 Ap^{(j)} \in  V_{j+1} , &  \quad  A p^{(j)} = \sum_{e=0}^{j +1}  c_{e} p^{(e)}  \\
 \left<r^{(k+1)} , p^{(j)}  \right> _{A} &=  \sum_{e = 0}^{ j=1 }  \left<r^{(k+1)}, c_{e}p^{(e)} \right> 
   \end{split} 
 \] 

 Chosing the search directions like this is corresponding to the Conjugate gradient method. 

 \subsection{Conjugate Gradient Method Algorithm}%
 \label{sub:conjuaget_gradient_method_algorithm}

 \begin{align*}
   x^{(0)} \quad  & \text{is given}  \\
   r^{(0)} &=   b - A\cdot x^{(0)}  \\
   p^{(s)} &=   r^{(s)}  \\
   \text{For } k&=   0,1,2, \ldots \\
    &   \begin{cases}
    \alpha &=   \frac{p^{(k)T} r^{(k)}}{{p^{(k)}}^{T} A p^{(k)}}  \\
     x ^{(k+1)} &=  x ^{(k)} + \alpha  p^{(k)}  \\
     r^{(k+1)} &=  r^{(k)} - \alpha _{k} A p^{(k)}  \\
     \beta _{k} &=  \frac{\left( A p^{(k)} \right)^{T} r^{(k+1)}}{ \left( Ap^{(k)} \right)^{T} p^{(k)}}   \\
     p^{(k+1)} &=  r^{(k+1)} - \beta _{k} p^{(k)} 
   \end{cases} 
 .\end{align*}
 
 \subsection{Simplification }%
 \label{sub:simplfication_}
 We want to simplify the expression for $\alpha _{k}  $ and $\beta  _{k}$
  \begin{align*}
    p^{(k+1)} &=  r^{(k+1)} - \beta  _{k} p^{(k)} \\
    p^{(k)} &=  r^{(k)} - \beta _{k-1} p^{(k-1)} \quad \implies \text{multiply} \quad  r^{(k)T}    \\
  r^{(k)T} p^{(k)} &= \|r^{(k)}\|_{2}^{2} - \beta _{k-1} r^{(k)T} p^{(k-1)} \\
  \text{So} \quad  &\alpha _{k} =  \frac{\|r^{(k)}\|_{2} ^{2} }{ p^{(k)} A p^{(k)}}     \\
  r^{(k+1)T} p^{(k+1) } &=   \|r^{(k+1)}\|^2 - \beta _{k} r^{(k+1)T} p^{(k)}  \\
  r^{(k)T} p^{(k+1)} &=  - \beta _{k} r^{(k)T} p^{(k)} = - \beta _{k} \|r^{(k)}\|^2 = \|r^{(k+1)}\|^2 
\end{align*}
In the end is the results \[
\beta _{k} = - \frac{\|r^{(k)}\|^2}{\|r^{(k+1)}\|^2} \quad \text{and} \quad    \alpha _{k} =  \frac{\|r^{(k)}\|_{2} ^{2} }{ p^{(k)} A p^{(k)}}    
\] 

\subsection{Modified algorithm}%
\label{sub:modified_algorithm}
\[
  \begin{split}
p^{(0)} &=   r^{(0)}  \\
r_{l} &=   \| r^{(0)}\|^2 \\
\text{For} \quad    &  k = 0,1,2,\ldots \\
 & 
\begin{cases}
v&= Ap^{(k)}  \\
t &=  p^{(k)T }  v \quad \quad  \quad \quad      \to  \text{saxpy}  \\
\alpha _{k} &=  \frac{r_{l}}{t} \quad \quad \quad \quad \quad \quad       \to  \text{saxpy}    \\
x^{(k+1)} &=  x^{(k)} + \alpha _{k} p^{(k)} \quad \quad \to  \text{inner product}   \\
r_c &=  \| r^{(k+1)}\| ^2  \quad \quad \quad    \to   \text{saxpy} \\
p^{(k+1)} &=  r^{(k+1)} + \frac{rc}{r_{l}}    p^{(k)} \quad \quad    \to  \text{saxpy}  \\
r_{l}   &  \leftarrow r_{c}
\end{cases}
  \end{split} 
\] 
Operations done in the numerical method \[
A \times  \text{vector} \quad  \left[ B\left( h^2 \right) \quad \text{for full matrices }  \right]  
\] 

\subsection{Convergence of the Conjugate Gradient Algorithm}%
\label{sub:convergence_of_the_conjugate_gradient_algorithm}

Since all search directions are mutually $A$- orthogonal. theu are linearly indepedent. After $n$ iterations, they span all of $\mathbb{R} ^{n}$.. Since the residuals $r^{(n)} $ is orthogonal to all of $r^{(0)} , \ldots, r^{(n-1)}$ must be $0$ and therefore \[
0 = r^{(0)n} = b - Ax^{(n)} \to  Ax^{(n)} = b 
\] 
But then the algorithm is only competitive when it terminates in $k \ll n$ iterations with a sufficient accurate solution.

\begin{theorem}
  Let $A$ be SPD. The error after $k$ iterations is bounded as \[
  \|e^{(j)}\|_{A } \le \frac{2c^{k}}{1+ c^{2k}} , \quad c = \frac{\sqrt{K_{2}\left( A \right)} - 1}{ \sqrt{K_{2}\left( A \right)}  +1}  
  \] 
\end{theorem}
\begin{remark}
  $\|v\|_{A} = \sqrt{ v^{T} A v} $
\end{remark}


 
\subsection{Next Lecture Hint}%
\label{sub:next_lecture_hint}

Next lecture will be about precondition. We solve $Ax =b$ . An equivalent formulation is to pick an invertible $P$ and solve \[
  \begin{split}
P^{-1} Ax &=   x P^{-1} \\
\hat{A} x = \hat{b}
  \end{split} 
\]  
\textbf{Criteria } 
\begin{enumerate}
  \item Let $P$ approximate $A$ 
  \item Should be cheap to solve systems \[
  P^{-1} y = c
  \] 
\end{enumerate}

\newpage
\section{Lecture 01/09/20}%
\label{sec:lecture_01_09_20}

Well posedness of the inital value problem. \[
\dot{y} = f\left( t,y \right), \quad y\left( 0 \right) = y_{0} 
\] 
Is stable on $\left[ 0,T \right] $ if for any sufficiently small $\varepsilon > 0$ m there are $\left( \delta _{0}, \delta \left( t \right) \right)$ s.t.\[
\|\delta _{0}\|_{}^{} < \varepsilon  , \quad  \|\delta \left( t \right)\|_{}^{} < \varepsilon  , \quad  t \in  \left[ 0,T \right] 
\] 
Such that $\|y\left( t \right) - z\left( t \right)\|_{}^{} < C\cdot \varepsilon \quad  \forall t \in \left[ 0,T \right]  $  for some constant $C$. $z\left( t \right)$ solves the IVP \[
\dot{z} \left( t \right) = f\left( t, z\left( t \right) \right) + \delta \left( t \right) , \quad  z_{0} = y_{0} + \delta _{0} 
\] 
One can prove that if $f$ is Lipschitz (constant L), then the solution is stable $C= (1+T) e^{tL}$ . 

\subsection{Flow of a vector field}%
\label{sub:flow_of_a_vector_field}

For us a vector field is a continuous map \[
f: \mathbb{R} ^{m} \to  \mathbb{R} ^{m}
\] 
For a fixed value of $t$ consider the map $\phi _{t,f}\left( y_{0} \right)  = y\left( t \right)$ is called the $t$-flow of the vector field $f$. Its domain of definition may depend on $t$ and $f$.
\begin{tcolorbox}
  Suppose that \[
    \phi _{t_{1}, f} \left( \phi _{t_{2} , f} \left( y_{0} \right) \right) = \phi _{t_{1}, t_{2}, f} \left( y_{0} \right)
  \] 
  and \[
  \phi _{-t_{1}, f} \left( \phi _{t_{1}, f}\left( y_{0} \right) \right) = \phi_{0, f} \left( y_{0} \right) = y_{0} \quad  \implies  \quad  \left( \phi _{t, f} \right)^{-1} = \phi _{-t, f}  
  \] 
  Typical notation: \[
   \phi _{t,f}\left( y_{0} \right) = e^{tf} y_{0}
  \]   
\end{tcolorbox}

\subsection{Numerical Integration of ODE}%
\label{sub:numerical_integration}
Always assume a finite time interval $\left[ 0,T \right]$.

Vector field $\left( f \right)$ does not contain parameters.  Split the finite interval into subintervals \[
t_{0} < t_{1} < \ldots < t^{N} = T
\] 
Often we assume $t_{n} = t_{0} + nh$ , where $h$ is the stepsize. Also we may have $h = \frac{T - t_{0}}{ N} $ . For $t = t_{j}$, let $u_{j} \approx y\left( t_{j} \right) = y_{j}$ and $f_{j} f\left( t_{j}, y_{j} \right)$


\newpara
Two classes of methods 
\begin{enumerate}
  \item One-step method , $u_{n+1} = \chi _{n+1} \left( u_{n} \right)$
  \item Multistep methods, $u_{n+1}$ depends on $u_{n}, u_{n-1}, \ldots , u_{n  k+1}$  and also $f_{n+1}, f_{n}, \ldots, f_{n-k+1}$
\end{enumerate}

\subsubsection{The Simplest Schemes}%
\label{ssub:the_simplest_schemes}

\begin{enumerate}
  \item \textbf{Euler} , $u_{n+1} = u_{n} + h f\left( t_{n} , u_{n} \right)$ 
  \item \textbf{Backward Euler (Implicit Euler)}  \[
  u_{n+1} = u_{n} + h f\left( t_{n+1}, u_{n+1} \right)
  \] 
\item \textbf{Trapezoid rule} . \[
    u_{ n+1}  = u_{n} + \frac{h}{2}  \left( f\left(  \right) \right)
\] 
\item Midpoint rule  \[
u_{n+1} = u_{n} 0 h f\left( t_{n} + \frac{h}{2},  \frac{y_{n} + y_{n+1}}{2}  \right)
\] 

\end{enumerate}

\subsubsection{The most important classes of one-step schemes}%
\label{ssub:the_most_important_classes_of_one_step_schemes}

Taylor Series methods. \[
\dot{y} = f\left( t, y \right) , \quad  y\left( 0 \right) = y_{0} , \quad  y\left( t \right) \in  \mathbb{R} ^{m}  \\
\]  
If $y\left( t \right)  $ is sufficiently smooth then we can compute the taylor expansion such that \[
y \left( t+ h \right) = y\left( t \right) h \dot{y} \left( t \right) + \frac{1}{2} h^2 \ddot{y}\left( t \right) + \ldots + \frac{1}{q!} h^{q} y^{(q)} + R_{q+1}  .
\] 
where \[
R_{q+1} = \frac{y^{(q+1) }\left( \zeta  \right)}{ \left( q +1 \right) !}  , \quad  \zeta  \in  \left( t, t+h \right) 
\] 
Compute $y ^{(k)} \left( t \right)$ , $k>1$ from diff eq. \[
  \begin{split}
\ddot{y} \left( t \right)  & = \frac{d }{dt } f\left( t, y\left( t \right) \right) = \frac{\partial }{\partial t} f \left( t, y\left( t \right) \right) + \frac{\partial f}{\partial y}  \left( t, y\left( t \right)   \right) \dot{y}\left( t \right)  \\
 & = \frac{\partial f}{\partial t}  \left( t, y\left( t \right) \right) +\frac{\partial f}{\partial y} \left( t, y\left( t \right) \right) \cdot f\left( t, y\left( t \right) \right)
  \end{split} 
\] 
Similary we can compute \[
y^{(3)}\left( t \right) , y^{(4) }\left( t \right) , \ldots
\]  
and plug this into the taylor expansion and ignore the remainder term. 

\newpara
\textbf{Example.} 
Let $\dot{y} = y^2$ and $y\left( 0 \right) = y_{0}$ such that \[
\ddot{y} = 2y \dot{y} = 2y^3, \quad y^{(3) } = 6 y^2\dot{y} = 6y^{4} , \quad y^{(k)} = k! y^{(k+1)}  
\] 

This can be plugged into the taylor expansion such that \[
y\left( t + h \right) = y\left( t \right) + h y\left( t \right)^2 + \frac{1}{2} h^2 2 y\left( t \right)^2 + \ldots = y\left( t \right) + hy\left( t \right)^2 + \ldots
\]  
which ends up with the method \[
y_{n+1} = \sum_{k=1}^{ q}  h^{k-1} y ^{k}_{n}
\] 

\subsubsection{Runga kutta methods}%
\label{ssub:runga_kutta_methods}

Generalization class of methods. COntains Euler, Backwards Euler, Trapozoidal Rule, Midpoint and more. The format is described like this \[
  \begin{split}
K_{i}  & = f\left( t _{n} + c_{i}h, u_{n} + \sum_{j=1}^{s} a_{ij} K_{j}  \right) \quad  i =  1, \ldots, s  \\
u_{n+1}  & = u_{n} + h \sum_{i=1}^{s}  b_{i} K_{i}
  \end{split} 
\] 

\begin{itemize}
  \item \textbf{Explicit RK Methods} . $a_{ij} = 0, \quad j \ge i $
  \item \textbf{Butcher Tablaeuz} 
\begin{table}[htpb]
  \centering
  \caption{Butcher Tablaeuz}
  \label{tab:label}
  \begin{tabular}{l|ccc}
    $c_{1}$  & $a_{11} $ & $\ldots$ & $a_{1s}$ \\
    $\vdots $ \\
    $c_{s}$ & $ a _{s1}$ & $\ldots$ & $a_{ss}$ 
     & \hline \\
            & $b_{1}$ & $\ldots$ & $b_{s}$
  \end{tabular}
\end{table}

\end{itemize}

\subsection{Analysis of one-step methods}%
\label{sub:analysis_of_one_step_methods}

The exact solution does not obey the numerical formula. We write the formula as \[
  \begin{split}
u_{n+1}   &  = u_{n} + h \phi _{h,f} \left( t_{n} , u_{n} \right) \\
y_{n+1} &= y_{n} + h \phi _{h,f} \left( t_{n}, y_{n} \right) + \varepsilon _{n+1} \\
  \end{split} 
\] 
We set $\varepsilon _{n+1} = h \tau _{n+1} \left( h \right)$ where $\tau _{n+1} \left( h \right)$ is the local trunction error. 

\newpara
Define also \[
\tau \left( h \right) = \max_{n} \|\tau _{n+1} \left( h \right)\|_{}^{}
\] 
A method is called \textbf{consistent}  if $\lim_{h\to 0} \tau \left( h \right) = 0$. If $\tau \left( h \right) = O\left( h ^{p} \right)$ as $h \to  0$ then ot has order (of consistency) $p$.




\newpage
\section{Lecture 2020-09-15}%
\label{sec:lecture_2020_09_15}

\textbf{Local Error and order conditions} 

Multistep methods - $k$-step.
\begin{equation}
\label{eq:mul1}
  \begin{split}
  \alpha _{k} u_{n+k} + \alpha _{k-1} u_{n+k} + \ldots + \alpha _{0} u_{n}  & = h\left( \beta _{k} f_{n+k} + \ldots + \beta _{0} f_{n} \right)   \\
  \quad  &    \quad \alpha _{0} \text{ or } \beta _{0} \neq 0, \alpha _{k} \neq 0 \quad \text{often } \alpha _{k} = 1    
  \end{split} 
\end{equation}
 

We have the local arror $y_{k} - u_{k}$ . where we assume $u_{j} = y_{j} , j = 0, \ldots, k-1$ (exact input values). For any multistep method define \[
L\left( y, t, h \right) =  \sum_{i=0}^{k} \left( \alpha _{i} y\left( t+ ih \right) - h \beta _{i} y' \left( t + ih \right) \right) \\
\] 


From the formula \eqref{eq:mul1} we get \[
  \begin{split}
\sum_{i=0}^{k-1}  \left( \alpha _{i} y_{i} - h \beta _{i}f\left( t_{i} , y_{i} \right) + \alpha _{k} u_{k} - h \beta _{k} f\left( t_{k} , u_{k} \right) \right)  & = 0 \\
L\left( y, t_{0}, h \right) =  \alpha _{k} \left( y_{k}- u_{k}   \right) - h \beta _{k}\left( f\left( t_{k} , y_{k} \right) f\left( t_{k}, u_{k} \right) \right)  &  \\
  \end{split} 
\]  
By taylor expansion \[
  \begin{split}
L\left( y, t_{0},h \right) &=  \left( \alpha _{k} I - h \beta _{k} \frac{\partial f}{\partial y} \left( t_{k}, \mu  \right) \right) \left( y_{k} - u_{k} \right) \\
y_{k} - u_{k} &=  \left( \alpha _{k} I - h \beta _{k} \frac{\partial f}{\partial y} \left( t_{k}, \mu  \right) \right) ^{-1} L\left( y, t_{0}, h \right)\\
\mu &=  \theta u_{k} + \left( 1- \theta  \right) y_{k}, \quad  \theta  \in  \left( 0,1 \right)  \\
  \end{split} 
\] 
For the local error : $y_{k} - u_{k} = \frac{1}{\alpha _{k}}  L \left( y_{}, t_{0}, h \right) \left( 1 + O\left( h \right) \right)$

\newpara
\subsection{Order of linear multistep methods.}%
\label{sub:order_of_linear_multistep_methods_}

Two equivalent definition of order 
 \begin{itemize}
   \item For any sufficuent smooth function $y\left( t \right)$, we have $L\left( y,t,h \right) = O\left( h^{p+1} \right)$
   \item The local error is $y_{k}- u_{k} = U\left( h^{p+1} \right)$.
 \end{itemize}

 \begin{definition}
   The two polynomials $\rho \left( z \right), \sigma \left( z \right)$ \[
     \begin{split}
   \rho \left( z \right)  & = \alpha _{k} z^{k} + \alpha _{k+1} z^{k+1} + \ldots + \alpha _{0}  \\
   \sigma \left( z \right) &=  \beta _{k}z^{k} + \beta _{k-1} z^{z-1} + \ldots + \beta_{0}  \\
     \end{split} 
   \] 
 \end{definition}

 \begin{theorem}
   The method has order $p$ if one of the following equivalent conditions is satisfied.
   \begin{enumerate}[label=(\roman*)]
     \item $\sum_{i=0}^{k}  \alpha _{i} = 0$ and $\sum_{i=0}^{k}  \alpha _{i} i^{q} = q \sum_{i=0}^{k} \beta _{i} i^{q-1} , \quad  q = 1, \ldots, p    $
     \item $\rho \left( e^{h}  \right) - h \sigma \left( e^{h} \right) = O\left( h^{p+1} \right)$ as $h\to 0$
     \item $\displaystyle \frac{\rho \left( z \right)}{ \ln z}  - \sigma \left( z \right) = O\left( \left( z-1 \right)^{p} \right)$ as $z\to 1$
   \end{enumerate}
 \end{theorem}

 \begin{proof}
   \begin{enumerate}[label=(\roman*)]
     \item
  \[
    \begin{split}
 L\left( y,t, h \right)  & =  \sum_{i=0}^{k}  \left( \alpha _{i} \sum_{q \ge 0}^{}  \frac{i^{q} h^{q}}{ q!} y^{(q)} \left( t \right) - h \beta _{i} \sum_{r\ge 0}^{} \frac{i^{r} h^{r}}{r!} y^{(r+1)} \left( t \right)   \right) \\
 &= \sum_{i=0}^{k}  \alpha _{i}y\left( t \right) + \sum_{q\ge1}^{} \frac{h^{q}}{q!} y^{(q)} \left( t \right) \left( \sum_{i=0}^{k}  i^{q} - q \sum_{i=0}^{k}  \beta _{i} i^{q-1} \right)   \\
 &= O\left( h^{p+1} \right) \quad \to  \sum_{i=0}^{k}  \alpha _{i} = 0\text{ and } \\
  &\quad \quad \quad \quad \quad \quad \quad \quad \sum_{i=0}^{k}  \alpha  _{i} i^{q} - q   \sum_{i=0}^{k}  \beta _{i} i^{q-1}   = 0 , \quad  q = 1, \ldots, p       \\
    \end{split} 
 \] 
   \end{enumerate}
 \end{proof}

\begin{tcolorbox}
  \textbf{Example.}  2-step explicit Adams \[
    \begin{split}
    u_{n+2} - u_{n+1} &=  h\left( \frac{3}{2} f_{n+1} \frac{1}{2} f_{n} \right) \\
    \alpha _{2} = 1, \quad  \alpha _{1} = -1, \alpha _{0} = 0,  &  , \beta _{2} = 0, \beta _{1} = \frac{3}{2} , \beta _{0} = -\frac{1}{2} 
    \end{split} 
  \] 
  \begin{itemize}
    \item $q=0$ \[
    \sum_{i=0}^{2}  \alpha _{i} = 0 -1 + 1 = 0 
    \] 
  \item $q=1$ \[
  \sum_{i=0}^{2}  i \alpha _{i} - 1 \sum_{i=0}^{2}  \beta _{i} i^{0} = \left( -1 +2 \right) - \left( \frac{3}{2} - \frac{1}{2} \right) = 0
  \] 
\item $q = 2$ \[
\sum_{i=0}^{2} i^2 \alpha _{i} - 2 \sum_{i=0}^{2}  i \beta _{i} = \left[ -1 +  2^2 \cdot 1\right]  - 2 \left[ 0 \cdot \left( -\frac{1}{2} \right) + 1\cdot \frac{3}{2} \right] = 0
\] 
\item $q = 3$  \[
\sum_{i=0}^{2}  i^3 \alpha _{i}  - 3  \sum_{i=0}^{2}  i^2 \beta _{i} = \left[ -1 + 2 ^3 \cdot 1 \right] - 3 \left[ 1 \cdot  \frac{3}{2} \right] = \frac{5}{2} \neq 0
\] 
  \end{itemize}
  The method has order $p=2$
\end{tcolorbox}

\begin{definition}
  
\textbf{Error constant}. We found in the proof \[
L\left( y,t,h \right) = \sum_{q \ge 0}^{}  C q h^{q} y^{(q)}\left( t \right) 
\] 
and $p \implies  C_{0} = C_{1} = \ldots = C_{p} = 0$, where $C_{p+1}$ is called \textbf{the error constant} .
\end{definition}

\begin{definition}
  \textbf{Consistency.}  (11.6   ). A multistep method is consistent if $p \ge 1$. $C_{0} = C_{1} = 0$. Can be formulated as \[
    P\left( 1 \right) = 0, \quad  P'  \left( 1 \right) = \sigma \left( 1 \right) 
  \] 
  Usual assumption: $P\left( z \right)$ and $\sigma \left( z \right)$ have no common factors \[
  p\left( z \right) = \alpha _{k}\left( z-r \right) \ldots \left( z - r_{k} \right) , \quad  \sigma \left( z \right) = \beta  _{k} \left( z-s_{1} \right) \ldots \left( z - s_{k} \right)  
  \] 
  Then $r_{i} \neq s_{j}$ for all $i,j$.
\end{definition}

\subsection{Difference equations and (zero) stability}%
\label{sub:difference_equations_and_zero_}

Consider the multistep method \[
u_{n+2} + 4 u_{n+1} - 5 u_{n} = h\left( 4 f_{n+1} + 2 f_{n} \right)
\] 

Check order \[
  \begin{split}
c_{0}  & = \sum_{}^{}  \alpha _{i} = 0 \\
c_{1} &=  \left( 4 +2 \right) - \left( 4+2 \right) = 0 \\
c_{2} &=  \left( 4 +4 \right) - 2 \left( 1\cdot 4  \right) = 0 \\
c_{3} &=  \left( 4 + 8  \right) - 3\cdot \left( 1\cdot 4 \right) = 0 \\
c_{4} &=  \left( 4 + 16 \right) - 4 \cdot \left( 1\cdot 4 \right) = 4 \\
  \end{split} 
\] 
Apply method  to the problem $y'  = y$ with $y\left( 0 \right) = 1$ 
\begin{equation}
\label{eq:this2}
  \begin{split}
u_{n+2} + 4u_{n+1} - 5 u_{n}  & = h\left( 4 u_{n+1} + 2 u_{n} \right) \\
u_{n+2} + 4\left( 1 -h \right) u_{n+1} - \left( 3 + 2h \right) u_{n} &= 0 \\
  \end{split} 
.\end{equation}

Try to insert solution $y_{j} = z ^{j} $ in the equation into \eqref{eq:this2}  \[
z^{n+2} + 4 \left( 1 - h \right) z^{n+1} - \left( 3 + 2h \right) z^{n} &= 0 \\.
\] 
Ignore $z= 0$  \[
z ^2 + 4 \left( 1 -h \right) z - \left( 5 + 2h \right) =0
\] 
Two roots, $z_{1} $ and $z_{2}$. The general solution is \[
u_{n} = A\cdot z_{1} \left( h \right)^{n} + B z_{2}\left( h \right)^{n}
\] 
The roots \[
z_{1,2}  = -2 + 2h \pm \sqrt{4h^2 -  6h +9} 
\] 
Leads to $z_{1} = 1 + h + O\left( h^2 \right)$, $z_{2} = -5 + 3h + O\left( h^2 \right)$. 

\newpara
$z_{1}$ reflects the true solution, but $z_{2}$ is a spurious solution (falsk l√∏sning).  
\begin{remark}
  $ \left\lvert z_{2}\left( h \right) \right\rvert ^{n} \to  \infty$, even for small $h$.
\end{remark}
This method cannot converge even though we have seen that $P = 3$. 
 
\newpara
Consider $y'  = f\left( t,y \right) := 0$.  In general for a multistep method applied to $y'  = 0$ will give you \[
\sum_{j=0}^{k}  \alpha _{j} u_{n+j} = 0
\] 
Solve by setting $u_{i} = z^{i}$ \[
\sum_{j=0}^{k} \alpha _{j} z^{j} = 0 \quad \text{or}\quad  P\left( z \right) = 0  
\] 
First consistency condition says $P\left( 1 \right) = 0$.  so $z_{1} = 1$ is a root. We also get \[
z_{2} , \ldots , z_{k}  \text{ some may be multiple roots}
\] 
\[
  \begin{split}
  P\left( z \right)  & = \left( z- \xi _{1} \right)^{m_{1}} \left( z - \xi _{2} \right)^{m_{2}} \ldots \left( z - \xi _{\mu } \right) ^{ m_{\mu }} \\
  \sum_{i=1}^{\mu }  m_{i} &=  k \\
  \end{split} 
\] 
The general solution is \[
u_{n} = p_{1} \left( n \right) \xi ^{n}_{1} + \ldots + P _{\mu }\left( n \right) \xi ^{n}_{\mu }
\] 
where $p_{i}$ is a polynomial of degree $m_{i} -1$.

\begin{definition}
  The method is zero-stable  if $ P\left( z \right) $  
  \begin{enumerate}[label=(\roman*)]
    \item The roots $\xi _{i}$ satisfied $\left\lvert \xi _{i} \right\rvert  \le 1$
    \item for root $\xi _{i}$ such that $\left\lvert \xi _{i} \right\rvert  = 1$, then $m_{i} = 1$
  \end{enumerate}
\end{definition}



\newpage

\section{References}%
\label{sec:references}



  

\bibliographystyle{plain}
\bibliography{references}
\end{document}

