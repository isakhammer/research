\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Numerical Math}
\author{isakhammer }
\date{August}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{enumerate}
\usepackage{todonotes}


\usepackage{hyperref} 
\hypersetup{
  colorlinks=true, %set true if you want colored links
  linktoc=all,     %set to all if you want both sections and subsections linked
  linkcolor=blue,  %choose some color if you want links to stand out
} 
\hypersetup{linktocpage}


% inscape-figures
\usepackage{import}
\usepackage{pdfpages}
\usepackage{transparent}
\usepackage{xcolor}
\newcommand{\incfig}[2][1]{%
\def\svgwidth{#1\columnwidth}
\import{./figures/}{#2.pdf_tex} } \pdfsuppresswarningpagegroup=1

% Box environment
\usepackage{tcolorbox}
\usepackage{mdframed}
\newmdtheoremenv{definition}{Definition}[section]
\newmdtheoremenv{theorem}{Theorem}[section]
\newmdtheoremenv{lemma}{Lemma}[section]

\theoremstyle{remark}
\newtheorem*{remark}{Remark}


\begin{document}
\maketitle
\tableofcontents
\newpage

\newpage
\section{Linear Systems I}%
\label{sec:linear_systems_i}
\subsection{Gaussian Elimination}%
\label{sub:gaussian_elimination}

\begin{tcolorbox}
  Introduction of linear systems (nxn), Cramer's rule to compute solution vector
\end{tcolorbox}

For a matrix $A \in \mathbb{R}^{n\times n }$ to solve the system of equation $Ax = b$ must this equivalent statements hold ,
\begin{itemize}
  \item $A$ is invertible.
  \item $rank\left( A \right) = n$ 
    \item The homogenous system $Ax = 0$ only admits the null solution.
\end{itemize}

A way to compute the solution is formally prvided by \textbf{Cramers rule} 
\begin{equation}
\label{eq:cramers-rule}
x_{j} = \frac{\Delta_j}{det\left( A \right)} \quad j = 1,\ldots,n 
.\end{equation}
where $\Delta_j$ is the determinant of the matrix obtained by substituting the $j$-th column of $A$ with the right hand side $b$. This formula is, however, of little practical use. Indeed, if the determinant are evaluated by the recursive relation s.t the computation effort solving this is $\left( n+1 \right)!$

 \begin{tcolorbox}
   Reviewed GEM, rewrote GEM with help of matrix multiplication from the left (using Frobenius matrices), derived fundamental properties for upper and lower triangulate matrices (e.g. products and inversion of lower triangular matrices), stated and proved LU factorizations for matrices A with invertible leading principal submatrices up to order n-1.
 \end{tcolorbox}


\begin{definition}[Frobenius Matrix]
   A \textbf{Frobenius matrix}  is a square matrix with the following properties,
   \begin{itemize}
     \item All entries on the main diagonal are ones.
     \item The entries below the main diagonal of at most one column are arbitrary.
     \item Every other entry is zero.
   \end{itemize}
 \end{definition}

 Example of a Nobelium matrix is,
 \[
   A = 
   \begin{pmatrix}
     1 & 0 & 0 & \ldots & 0 \\
     0 & 1 & 0 & \ldots & 0 \\
     0 & a_{32} & 1 & \ldots & 0 \\
     \vdots & \vdots & \vdots  & \ddots \\
     0  & a_{n 2}  & 0  & \ddots  &  1
   \end{pmatrix} 
 \] 

 \par


 \begin{definition}[Upper and Lower triangular Matrices]
   Let $n$ be an integer , $n> 2$. The matrix $L \in \mathbb{R}^{n\times n}$ is said to be \textbf{Lower Triangular} if $l_{ij} = 0$ for every $i$ and $j$ with $1 \le i < j \le n$. The matrix $L \in \mathbb{R}^{n\times n }$ is calles the \textbf{unit lower triangular }  if it is lower triangular , and also the diagonal elements are all equal to unity, that is $l_{ii} = 1$  for $i = 1,2, \ldots , n$.
   \par 
   Let $n\ge 2 $.  The matrix $U \in \mathbb{R}^{n\times n}$ is said to be \textbf{upper triangular }  if $ u_{ij} = 0$ for every $i$ and $j$ with $1 \le j < i \le n$.
 \end{definition}
 \todo[inline]{ Make a overview of the properties they hold.  }
 
 Classes of matrices exist such that GEM can be always safely employed in its basic form. Among them, we recall the following case. 
 \begin{itemize}
   \item Matrices \textbf{diagonally dominant by rows} .
   \item Matrices \textbf{diagonally dominant by columns}. In such case one can even show that the multipliers are in module less than or equal to 1.
   \item Matrices symmetric and positive definite. 
 \end{itemize}

 THe GEM algorithm can be describes as follows. 
 \begin{align}
   m_{ik} &= \frac{a_{ik}^{\left( k \right)}}{a_{kk}^{\left( k \right)}} \quad i = k+1 , \ldots, n. \\ 
   a_{ij}^{\left( k+1 \right)} &= a_{ij}^{\left( k \right)} - m_{ik} a _{kj}^{\left( k \right)}, \quad i,j = k+1, \ldots, n \\ 
   b_{i}^{\left( k+1 \right)} &= b_{i}^{\left( k \right)} - m_{ik}b_{k}^{\left( k \right)}, \quad i = k+1 , \ldots, n    
 .\end{align}

 

\subsection{LU Factorization}%
\label{sub:lu_factorization}
\begin{tcolorbox}
  Split solution of linear system into 1) LU factorization, 2) forward substitution, 3) backward substitution, derived various ways to compute LU factorization directly, give example where pivoting might be necessary or avoids large errors,  discussed permutation matrices P and GEM algorithm with partial column pivoting, proved formally PA = LU factorization. 
\end{tcolorbox}




\begin{tcolorbox}
  Finished proof from last time (complete proof can be found under teaching material, lecture03), derived complexity of LU factorization ( $O(n^3)$  and forward/backward substitution algorithms ( $O(n^2)$, introduced symmetric and positive definite (spd) matrices and started to discuss Cholesky factorization	
\end{tcolorbox}


\begin{theorem}
  Let $A \in \mathbb{R}^{n\times n }$. The LU factorization of $A$ with $l_{ii}=1$ for $i = 1, \ldots , n$ exist and is unique if the principal sub matrices $A_{i}$ of $A$ of order $i  = 1, \ldots, n-1$ are non-singular.
  \end{theorem}  

  \begin{proof}
    The existence of the LU factorization can be proved following the steps of the GEM. Here we prefer ti pursuer an alternative approach, which allows for proving at the same time both existence and uniqueness and that will be used again in later sections.
    \par
    Let us assume that the principalsubmatrices $A_{i}$ of $A$ are nonsingular for $i = 1, \ldots, n-1$ and prove, by induction on $i$, that under this hypothesis the LU factorization of $A$ with $l_{ii} = 1$ for $i = 1, \ldots, n$ exists and is unique. 
    \par
    The property is obvious true if $i=1$. Assume therefore that there exists an unique LU factorization of $A_{i-1}$ on the form $A_{i-1} = L^{i-1} U^{i-1} $ with $l_{kk}^{i-1} = 1$  for $k = 1, \ldots , i-1$, and show that there exists an unique factorization also for $A_{i}$. We partition $A_{i}$ by block matrices as \[
    A_{i} = \begin{bmatrix} 
      A_{i-1} & c \\
      d^{T} & a_{ii}
    \end{bmatrix} 
    \] 
    and look for a factorization of $A_{i}$ on the form  \[
    A_{i} = L^{i} U^{i} = 
    \begin{bmatrix} 
      L^{i-1} & 0 \\
      \mathbf{l}^{T} & 1
    \end{bmatrix} 
    \begin{bmatrix} 
      U^{i-1} & u \\
      0^{T} & u_{ii}
    \end{bmatrix} 
    \] 
    Having also partitioned by blocks the factor $ L^{i}$ and $U^{i}$.  COmputing the product of these two factors and qeuating by blocks the element of $A_{i}$, it turn out that the vectors $\mathbf{l}$ and $u$ are the solution to the linear system $L^{i-1}u = c$ and $\mathbf{l}^{T} U^{i-1}$ = $d^{T}$. 
    \par
    On the other hand, since $det\left( A_{i-1} \right) =  det\left( L^{i-1} \right) det\left( U^{i-1} \right) \neq 0$, the matrices $L^{i-1}$ and $U^{i-1}$ are nonsingular and , as a result, $u$ and $\mathbf{l}$ exist and are unique.
    \par
    Thus, there exist a unique factorization of $A_{i}$, where $u_{ii}$ is the unique solution of the equation $u_{ii} = a_{ii} - \mathbf{l}^{T} u$. This completes the induction step of the proof.
    \par
    it now remains to prove that, if the factorization at hand exists and is unique,  then the first $n-1$ principall submatrices of $A$ must be nonsingular. We shall distinguish the case where $A$ is singular and when it is non-singular.
    \par
    Let us start from the second one and assume that the LU factorization of $A$ with $l_{ii} = 1$ for $i  =  1, \ldots, n$ ecists and is unique.  Then due to the previous equation of $A_{i} = L^{i} U ^{i}$ for $i = 1, \ldots, n$. Thus
    \begin{equation}
    \label{eq:Ai_1}
    det\left( A_{i} \right) = det\left( L^{i} \right) det\left( U^{i} \right) =  det\left( U^{i} \right) =  u_{11} u_{22} \ldots u_{ii}
    .\end{equation}
    from which, taking $i = n$ and $A$ nonsingular, we obstain $u_{11} u_{22} \ldots u_{ii} \neq 0$ for $i =  1, \ldots, n-1$. 
    \par
    Now let $A$ be a singular matric and assume that (at least) one diagonal entru of $U$ is qual to zero.  Denote by $u_{kk}$ the null entry of $U$ with minimum indec $k$. Thanks to the assumption of $A_{i}$, the fectiruzatuib can be cinouted without troubles until the $k+1$-th step. From that steo on, since the matric $U^{k}$ is singular, existence and uniqueness of the vector $\mathbf{l}^{T} $ are certainly lost, and, this, the same holds for the uniqueness of the factorization. In order tor
    this not to occur before the process has factorized the whole matrix $A$, the $u_{kk}$ entries must all be nonzero up to the index $k = n-1$, included, and this , due to \eqref{eq:Ai_1}, all the principal submatrices $A_{i}$ must be nonsingular for $k = 1, \ldots, n-1$ .

  \end{proof}



\subsection{Pivoting}%
\label{sub:pivoting}
\begin{tcolorbox}
  Stated and proved Cholesky factorization for spd matrices, discussed in detail an example of spd matrix from discrimination of a boundary value problem, introduced overdetermined system. Stated minimization property of orthogonal projections.
\end{tcolorbox}

\newpage
\section{Stability and Error Analysis}%
\label{sec:stability_and_error_analysis}

 


\subsection{Stability, forward and backward analysis}%
\label{sub:stability_forward_and_backward_analysis}

\begin{tcolorbox}
  Sources of error in numerical computations, reviewed machine representation of numbers using floating point numbers, introduced concept of local (absolute and relative) condition numbers, discussed computation of condition numbers for small data errors, considered linear system and introduced condition number of a matrix, programmed example using Hilbert matrix to illustrate effect of an ill-conditioning problem	
\end{tcolorbox}

\begin{definition}
  Consider a mapping $f$ from a subset $D$ of a normed linear space  $V$ with norm $\|\cdot \|_{V}$ into another normed linear space $W$ with norm $\|\cdot \|_{W}$ where $x \in D \subset V$ s. t. $f\left( x \right) \in W$. We shall be concerned with the sensitivity of the output to perturbation in the input; therefore, as a measure of sensitivity, we define the \textbf{absolute condition number} of $f$ by \[
    Cond\left( f \right) = \sup_{\substack{x,y \in D \subset V \\ x \neq y}} \frac{\|f\left( x \right) \le- f\left( y \right) f\left( x \right)\|_{W}}{\|y-x\|} 
  \] 
  If $Cond\left( f \right) = +\infty$ or if $ 1 \ll Cond\left( f \right) < \infty$, we say the mapping is \textbf{ill-conditioned}.
\end{definition}
 
\begin{definition}[Absolute Local Condition Number]
  When $\frac{\|f\left( y \right) - f\left( x \right)\|_{W}}{\|y-x\|_{V}}$ exhibits large variations as $\left( x,y \right)$ ranges through $D \times D $, it is more helpful to consider a finer local condition, \textbf{abslute local condition number}, as $x \in D \subset V$ of the function $f$, defined by \[
    \text{Cond}_x\left( f \right) = \sup_{\substack{\delta x \in  V \setminus \{0\}  \\ x + \delta x \in D}} { \frac{\|f\left( x + \delta x \right) -  f\left( x \right)\|_{W} }{\|\delta x\|_{V}} } 
  \] 
\end{definition}

\begin{definition}[Condition Number Matrix]
  A \textbf{condition number} of a matrix $A \in \mathbb{C}^{n\times n }$ is defined as 
\begin{equation}
\label{eq:condition_number}
K\left( A \right) =  \frac{\|A\|}{\|A^{-1}\|}
.\end{equation}
Where $\|\cdot\|$ is the induced matrix norm. 
\end{definition}

\begin{remark}
  Condition number of a singular matrix is set equal to infinity.
\end{remark}

\begin{definition}[Forward and backward analysis]
  The methods can be described as follows,  
  \begin{itemize}
    \item \textbf{Forward analysis}, which provides bound to the variations $\|\delta x_{n}\|$ on the solution due to both perturbation in the data and to errors that are intrinsic to the numerical method.
  \item \textbf{Backward analysis} , which aims at estimating the perturbations that should be "impressed" to the data of a given problem in order to obtain the results actually computed under the assumptions of working in exact arithmetic. Equivalently, given a certain computed solution $\hat{x_n}$, backwards analysis looks for the perturbations $\delta d_{n}$ on the data such that $F_{n} \left( \hat{x_n}, d_n + \delta_n \right)$. Notice that, when performing such an
    estimate, no account at al is taken into the way $\hat{x_{n}}$ has been obtained.
  \end{itemize}
\end{definition}
  
\subsection{Convergence}%
\label{sub:convergence}


In general are mathematical errors 
\begin{itemize}
  \item Error's due to the model, that can be controlled by a proper choice of the mathematical model . 
  \item Errors in the data, that can be reduced by enhancing the accuracy in the measurement of the data themself .
  \item Trunction errors, arising from having replaced the numerical model limits by operations that involve a finite number of steps.
  \item Rounding errors.
\end{itemize}

\newpage
\section{Linear Systems II}%
\label{sec:linear_systems_ii}

\subsection{Undetermined systems and Least-square Problems}%
\label{sub:undetermined_systems_and_least_square_problems}
\begin{tcolorbox}
  Show that the orthornormal projection into some subspace gives a the best-approximation of some given data in that subspace, used this connection to derive the normal equations solved by x iff x solves least-squares problem (l.s.p), discussed algorithm to l.s.p based on Normal equations and Cholesky factorization of AT A. Discussed possible disadvantages of this approach and introduced QR factorization of A as an alternative way to solve the l.s.p.	
  \par
  YAYEB: 3.1.1, 3.1.2, 3.2 until 3.2.1.

\end{tcolorbox}
\todo{ Seems like he lookes more into 3.4 and similar }


\begin{theorem}
  Suppose that $A \in \mathbb{R}^{m \times  n }$ where $ m \le n$.  Then, $A$ can be written in the form \[
  A = \hat{Q} \hat{R}
  \] 
  where $\hat{R}$ is an upper triangular $n \times n $ matrix and $\hat{Q}$ is an $m \times  n $ matrix which satisfies \[
  \hat{Q}^{T} \hat{Q} =  I_n
  \] where $I_n$ is the $n \times n $ identity matrix. If $rank\left( A \right) = n$ then is $\hat{R}$ nonsingular.
\end{theorem}


\subsection{Cholesky decomposition}%
\label{sub:chelesky_decomposition}
\begin{tcolorbox}
  Discussed full and reduced QR factorization. Recall Gram-Schmidt orthogonalization methods and reinterpreted it as a first method to compute the reduced QR factorization	
  \par
  YEB:3.4.3
\end{tcolorbox}

\subsection{Eigenvalue and Eigenvector computations}%
\label{sub:eigenvalue_and_eigenvector_computations}

\newpage
\section{Nonlinear Systems}%
\label{sec:nonlinear_systems}

 \subsection{Newtons method}
 \label{sub:newtons_method}

 \textbf{Newtons Method} Assuming that $f \in C^{1}\left( I \right)$ and that $f'\left( \alpha \right) \neq 0$ (i.e, $\alpha$ is a simple root of $f$  ), if we let \[
 q_{k} = f'\left( x^{(k)} \right) \quad \forall k \ge 0 
 \] 
 and assign the inital value $x^{(0)}$, we obstain the socalled Newtons method \[
 x^{(k+1)} = x^{(k)} - \frac{f\left( x^{(k)} \right)}{f'\left( x^{(k)} \right)} \quad k\ge 0 
 \] 


\subsection{Banach Fixed Point Iteration}%
\label{sub:banach_fixed_point_iteration}
\begin{tcolorbox}
  Briefly discussed conditioning of root finding problem, bisection method as a 1 dimensional method to compute a root, introduced the concept of convergence order for an sequence approaching a root, fixpoint iterations
  \par
  YEB: 6.1, 6.2.1, 7.1.5
  BLUB: 4.2.1 (pp.108-112)
\end{tcolorbox}

\par
The problem of fixed point iterations is often described like this \[
  \text{Given} \quad  \mathbf{G}: \mathbb{R}^{n} \to \mathbb{R}^{n}, \quad \text{find } x^{*} \quad \text{such that }  \mathbf{G} \left( \mathbf{x^{*}} \right) = \mathbf{x}^{*}   
\] 
Where $\mathbf{G}$ is related to $\mathbf{F}$ through the following property if $x^{*}$ is a fixed point of $\mathbf{G}$ then $\mathbf{F}\left( \mathbf{x^{*}} \right) = 0$ 

\begin{definition}[Contraction]
  A mapping $\mathbf{G}: D \subset \mathbb{R}^{n} \to \mathbb{R}^{n}$ is contractive on a set $D_0 \subset D$ if there exist a constant $\alpha < 1$ such that $\|\mathbf{G}\left( \mathbf{x} - \mathbf{G}\left( \mathbf{y} \right) \right)\| \le \alpha \|\mathbf{x} - \mathbf{y}\|$ for all $\mathbf{y}, \mathbf{x} $ in $D_0$, where $\|\cdot \|$ is a suitable vector norm.
\end{definition}


\begin{theorem}[Contraction-mapping theorem]
  Suppose that $\mathbf{G}: D \subset \mathbb{R}^{n} \to \mathbb{R}^{n}$ is contractive on a closed set $D_0 \subset D$ and that $\mathbf{G}\left( x \right) \subset D_0$ for all $\mathbf{x} \in D_0=$ . then $\mathbf{G}$ has a unique fixed point in $D_0$.  
\end{theorem}

\begin{proof}
  Let us first prove the uniqueness of the fixed point. For this assume that there exist two distinv fixed point $\mathbf{x}^{*}, \mathbf{y}^{*}$. Then \[
  \|\mathbf{x}^{*} - \mathbf{y}^{*}\| =  \|\mathbf{G} \left( \mathbf{x}^{*} \right) - \mathbf{G}\left( \mathbf{y}^{*} \right)\| \le \alpha \|\mathbf{x}^{*} - \mathbf{y}^{*}\|
  \] from which $\left( 1- \alpha \right) \|\mathbf{x^{*} - \mathbf{y}^{*}} \| \le 0$. Since $\left( 1- \alpha \right) > 0$, it must necessarily be that $\|\mathbf{x}^{*} - \mathbf{y}^{*}\| = 0$ , i.e. , $\mathbf{x}^{*} = \mathbf{y}^{*}$.
  \par  
  To prove the existence we show that $\mathbf{x}^{(k)}$ is a Cauchy sequence. This in turn implies that $\mathbf{x}^{(k)}$ is convergent to a point $\mathbf{x}^{(*)} \in D_0$. Take $\mathbf{x}^{(0)}$. Take $\mathbf{x^{(0)}}$ arbitary in $D_0$. Then, since the image of $\mathbf{G}$ is included in $D_0$, the sequence $\mathbf{x}^{(k)}$ is well defined as \[
  \|\mathbf{x}^{(k+1)} - \mathbf{x}^{(k)}\| = \|\mathbf{G}\left( \mathbf{x}^{(k)} \right) - \mathbf{G}\left( \mathbf{x}^{(k-1)} \right)\| \le \alpha \|\mathbf{x^{k}} - \mathbf{x}^{(k-1)}\|
  \] 
  After $p$ steps, $p\ge 1$ we obtain 
  \begin{align}
    \|\mathbf{x}^{(k+p)}- \mathbf{x}^{(k)}\| &\le \sum_{i=1}^{p} \|\mathbf{x}^{(k+i)} - \mathbf{x}^{(k+i-1)}\| \le \left( \alpha ^{p-1} + \ldots + 1 \right) \|\mathbf{x}^{(k+1)}- \mathbf{x}^{(k)}\| \\
                                             &\le \frac{\alpha ^{k}}{ 1- \alpha } \|\mathbf{x}^{(1)} - \mathbf{x}^{(0)}\|
  .\end{align}
  Owing to the continutity of $\mathbf{G}$ it follow that $\lim_{rk \to \infty} \mathbf{G}\left( \mathbf{x}^{(k)} \right) = \mathbf{G}\left( \mathbf{x}^{(*)} \right)$ Which proves that $\mathbf{x^{(*)}}$ is a fixed point of $\mathbf{G}$.


\end{proof}

\begin{tcolorbox}
  Implemented and test bisection methods, introduced concept of convergence order, implemented fixpoint iterations and tested it for 2 different fixpoint formulation of the same root finding problem 
  \par
  see L9
\end{tcolorbox}

\begin{tcolorbox}
  Finalize proof of contraction mapping theorem, consider convergence order, discussed how to estimate Lipschitz constant via derivatives. Introduced polynomial interpolation, Lagrange polynomials, discussed Lagrange form of the interpolation polynomial	
  BLUB: 4.2.1 (pp.108-112) and 6.2
\end{tcolorbox}
\subsection{Gauss-Newton Method for non-linear least squared problems}%
\label{sub:gauss_newton_method_for_non_linear_least_squared_problems}

\newpage
\section{Interpolation and Approximation}%
\label{sec:interpolation_and_approximation}

\subsection{Polynomial Interpolation}%
\label{sub:polynomial_interpolation}

\begin{tcolorbox}
  Implementing and plotting Lagrange polynomials, computing interpolation for some functions, Runge example, example of different nodes distribution (Chebyshev nodes)	
  YEB 8.1, BLUB 6.2
\end{tcolorbox}

\begin{tcolorbox}
   Introduce Newton's method, stated and proved local quadratic convergence. 	
   YEB 7.1

\end{tcolorbox}

\begin{tcolorbox}
  Newton polynomials, computing interpolation polynom in Newton form, n-th Newton divided difference	
  YEB 8.2, 8.2.1
\end{tcolorbox}

\begin{tcolorbox}
  Estimates of the interpolation error, condition of the interpolation problem	
   BLUB 6.2/6.3, YEYEB : 7-7.1.2
\end{tcolorbox}

\subsection{Aiken-Neville Algorithm}%
\label{sub:aiken_neville_algorithm}


\begin{tcolorbox}
  A recursion formula for the interpolationpolynomial is derived  which leads to the Aiken-Neville algorithm which evaluates an interpolation polynomial at a certain point	
  YEYEB: 7.1.2 (first 2 pages)

\end{tcolorbox}


\subsection{Orthogonal polynoms}%
\label{sub:orthogonal_polynoms}

\newpage
\section{Numerical Integration}%
\label{sec:numerical_integration}

\subsection{Integration Methods}%
\label{sub:trapezoid_rule}
\begin{tcolorbox}
  Integral as positive linearform, simplest examples of  quadrature rules (midpoint and trapezoidal rules), degree of exactness, Newton-Cotes formulas and estimates for quadrature error, composite trapezoidal rule (CTR)
  BLUB: 7.1-7.5

  (Similar and additional material in YEB:9 - 9.4)
\end{tcolorbox}

\subsection{Gauss-Christoffel Quadrature}%
\label{sub:gauss_christoffel_quadrature}
\begin{tcolorbox}
  Derived error estimate for CTR, implementation and convergence rate experiments, same for composite Simpson rule	
  BLUB:7.5
YEB:9.4
\end{tcolorbox}

\begin{tcolorbox}
  Derivation of quadrature rules of max order 2n+1 for weighted integrals, ortogonality properties for Gaussian quadrature rules, orthogonal polynomials,

I skipped some proofs and a theorem, please read proof of Lemma 9.16 (orthogonal polynomials of order n has n simple real roots)  and Thm 9.18 + proof (the weigths of the Gaussian quadrature are positive) in A3YEB
A3YEB: 9.3

\end{tcolorbox}

\begin{tcolorbox}
  Proved that there is no QR with n+1 points and order 2n+2, use Gram-Schmidt to orthogonalize polynomials, computed roots and weights for Gaussian quadrature rule with 3 nodes on [-1,1] and weight function w(x) = 1
None sources.

\end{tcolorbox}

\subsection{Romberg/ extrapolation}%
\label{sub:romberg_extrapolation}

\newpage
\section{Numerical ODE}%
\label{sec:numerical_ode}

\subsection{Runge Kutta Explicit }%
\label{sub:runge_kutta_explicit_}

\begin{tcolorbox}
  	First order ordinary differential equations as initial value proble and examples, derivation of explicit and implicit Euler's method via difference quotient and quadrature approach, definition of  one-step-methods (OSM) and their consistency and convergence order, stated and discussed main convergence theorem for OSM,
handnotes.
\end{tcolorbox}
 
\begin{tcolorbox}
  Derivation and formulation of Runge-Kutta methods, reformulation via stage-derivatives, definition of Butcher tableaus, implementation of RK method up to to order.
  None notes.
\end{tcolorbox}

\begin{tcolorbox}
  Proof of the main convergence theorem stated in Lecture 21, discrete Gronwall's inequality, autonomous systems and RKM, multivariate Taylor expansion, derivation of order condition up to p=3	
  handnotes.
\end{tcolorbox}

\subsection{Adams-Bashforth Methods}%
\label{sub:adams_bashforth_methods}

\subsection{Stiff Problems}%
\label{sub:stiff_problems}
\begin{tcolorbox}
  Stiff problems, implicit Runge-Kutta methods, stability function associated with RKM, A-stability	
  ODE-notes  Ch5
\end{tcolorbox}

\subsection{Linear Multi Step Method}%
\label{sub:linear_multi_step_method}

\begin{tcolorbox}
   Linear multistep methods (LMM), consistency order and conditions, null stability, main convergence theorem for LMMs, Examples of LMMs including Adam-Bashforth-Moulton and BDF	. ODE notes ch6
\end{tcolorbox}

 


\bibliographystyle{plain}
\bibliography{references}
\end{document}

